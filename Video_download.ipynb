{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VnlB4QZwt-w",
        "outputId": "a3c9f01c-9a6b-47a1-ff35-9c038f079bba"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "splits = {'train': 'data/train.json', 'validation': 'data/val.json', 'test': 'data/test.json'}\n",
        "df = pd.read_json(\"hf://datasets/hbfreed/Picklebot-2M/\" + splits[\"train\"])\n",
        "main_df = df.__deepcopy__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #Get the pitchers with the most FF pitch after 2022\n",
        "df_dev = df[df['pitch'] == 'FF']\n",
        "df_dev = df_dev[df_dev['date'] >= '2022-01-01']\n",
        "df_dev = df_dev.groupby('pitcher').size().reset_index(name='count')\n",
        "print(df_dev)\n",
        "# Save the top 50 pitchers to a CSV file\n",
        "# df_dev.to_csv('top_50_pitchers_ff.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "lNrCE0HkxH0I",
        "outputId": "f1412af3-0aef-4d72-d65b-5e93295c0c0b"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#get pitcher names who have thrown the most pitches\n",
        "df = df[df['pitch'] == 'FF']\n",
        "df = df[df['date'] >= '2022-01-01']\n",
        "top_pitchers = df['pitcher'].value_counts().head(10)\n",
        "print(\"Top 10 pitchers by number of pitches thrown:\")\n",
        "print(top_pitchers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KdGc3pOxKQL"
      },
      "outputs": [],
      "source": [
        "#get all unique pitch types\n",
        "pitch_types = df['pitch'].unique()\n",
        "print(\"Unique pitch types:\", pitch_types)\n",
        "#filter for only pither Blevins, Jerry(L)\n",
        "df = df[df['pitcher']=='Snell, Blake(L)']\n",
        "\n",
        "#Unique pitch types: ['FF' 'KC' 'SI' 'SL' 'CU' 'CH' 'FC' 'FA' 'ST' 'FS' '' 'SV' 'KN' 'EP' 'SC'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPoywu2lxT_F",
        "outputId": "3b926513-e0f3-4d62-d020-79aca5ce4e0a"
      },
      "outputs": [],
      "source": [
        "#get size\n",
        "print(df.shape)\n",
        "#get the data type of mph\n",
        "print(df['mph'].dtype)\n",
        "#convert mph to a float from a string\n",
        "df['mph'] = pd.to_numeric(df['mph'], errors='coerce')\n",
        "df = df.dropna(subset=['mph'])\n",
        "print(df.shape)\n",
        "print(df['mph'].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "r_g7h1l6xpkD",
        "outputId": "32c97371-ef2a-4da4-8314-99da7cd7a226"
      },
      "outputs": [],
      "source": [
        "#get counts of pitch_result\n",
        "df['pitch_result'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#order df by date\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df.sort_values(by='date', ascending=False)\n",
        "#print the first 50 dates\n",
        "print(\"First 50 dates in descending order:\")\n",
        "print(df['date'].head(50))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St67eu_DXEkH"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_video_url(play_id):\n",
        "    url = f\"https://baseballsavant.mlb.com/sporty-videos?playId={play_id}\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    video_tag = soup.find(\"video\")\n",
        "    if video_tag:\n",
        "        source_tag = video_tag.find(\"source\")\n",
        "        if source_tag and source_tag.get(\"src\"):\n",
        "            return source_tag[\"src\"]\n",
        "    return None\n",
        "\n",
        "def get_video(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    video_tag = soup.find(\"video\")\n",
        "    if video_tag:\n",
        "        source_tag = video_tag.find(\"source\")\n",
        "        if source_tag and source_tag.get(\"src\"):\n",
        "            return source_tag[\"src\"]\n",
        "    return None\n",
        "\n",
        "def download_video(video_url, output_file=\"temp.mp4\"):\n",
        "    r = requests.get(video_url, stream=True)\n",
        "    with open(output_file, \"wb\") as f:\n",
        "        for chunk in r.iter_content(chunk_size=1024*1024):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "def video_to_numpy(video_path, resize=None, max_frames=None):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if resize:\n",
        "            frame = cv2.resize(frame, resize)\n",
        "        frames.append(frame)\n",
        "        count += 1\n",
        "        if max_frames and count >= max_frames:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "def save_numpy_array(array, filename=\"video_frames.npy\"):\n",
        "    np.save(filename, array)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usVqfTqK-jXH"
      },
      "source": [
        "Prompt:\n",
        "The seconds when the pitcher is possibly in the middle, and the location of the pitchers change depending on the video. We may have to use ML for this.\n",
        "\n",
        "\n",
        "\n",
        "ChatGPT said:\n",
        "You're absolutely right ‚Äî since the pitcher‚Äôs timing and location vary from video to video, a fixed crop won't work well. This is a classic use case for computer vision with machine learning. Here‚Äôs a structured way to approach it:\n",
        "\n",
        "‚úÖ Goal:\n",
        "Automatically detect and crop the pitcher across different videos with varying camera angles and timings.\n",
        "\n",
        "üß† Solution Options:\n",
        "Option 1: Pose Estimation (Recommended)\n",
        "Use a pre-trained pose estimation model like:\n",
        "\n",
        "OpenPose\n",
        "\n",
        "MediaPipe Pose (Google)\n",
        "\n",
        "YOLO + Keypoint detection\n",
        "\n",
        "‚úÖ Pros:\n",
        "\n",
        "Accurate for identifying human bodies.\n",
        "\n",
        "Can isolate the pitcher based on pose and movement patterns.\n",
        "\n",
        "Option 2: Object Detection\n",
        "Train or use a model (like YOLOv5, YOLOv8, or Detectron2) to detect ‚Äúpitcher‚Äù as a custom class.\n",
        "\n",
        "‚úÖ Pros:\n",
        "\n",
        "Works on frame-by-frame basis.\n",
        "\n",
        "More precise than pose estimation in cluttered scenes.\n",
        "\n",
        "‚ùå Cons:\n",
        "\n",
        "Requires labeled data (you may need to annotate a small dataset of frames showing pitchers).\n",
        "\n",
        "üîß Workflow Example (with MediaPipe Pose in Colab):\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose()\n",
        "\n",
        "def detect_pitcher_bbox(video_path, max_frames=100):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    bboxes = []\n",
        "\n",
        "    for i in range(max_frames):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        h, w, _ = frame.shape\n",
        "\n",
        "        # Run pose estimation\n",
        "        results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        if results.pose_landmarks:\n",
        "            # Get min/max of landmark coordinates for bounding box\n",
        "            x_coords = [lmk.x for lmk in results.pose_landmarks.landmark]\n",
        "            y_coords = [lmk.y for lmk in results.pose_landmarks.landmark]\n",
        "            xmin, xmax = min(x_coords), max(x_coords)\n",
        "            ymin, ymax = min(y_coords), max(y_coords)\n",
        "\n",
        "            # Convert to pixel coordinates\n",
        "            x, y = int(xmin * w), int(ymin * h)\n",
        "            width, height = int((xmax - xmin) * w), int((ymax - ymin) * h)\n",
        "            bboxes.append((x, y, width, height))\n",
        "\n",
        "    cap.release()\n",
        "    return bboxes\n",
        "This gives you approximate bounding boxes of the pitcher from the first max_frames frames.\n",
        "\n",
        "üß™ Next Steps:\n",
        "You can either:\n",
        "\n",
        "Crop each frame using the detected bounding box.\n",
        "\n",
        "Track the pitcher across frames.\n",
        "\n",
        "Save the cropped regions into a new video or a NumPy array.\n",
        "\n",
        "Let me know if you want help setting up pose-based cropping, YOLO training with pitcher labels, or a pipeline to auto-crop and save video segments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s-5hli1tzeT"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "\n",
        "# Load YOLOv8 model once globally, force to use GPU if available\n",
        "model = YOLO('yolo11n.pt')\n",
        "if hasattr(model, 'to'):\n",
        "    model.to('cuda')\n",
        "\n",
        "def detect_and_draw_all_humans(frame):\n",
        "    \"\"\"\n",
        "    Detects all humans in the frame using YOLOv8.\n",
        "    Crops 20% off each horizontal side before detection for focus.\n",
        "    Returns:\n",
        "        - The original frame with bounding boxes drawn.\n",
        "        - A list of bounding boxes: (x1, y1, x2, y2, 'person') in original frame coordinates.\n",
        "    \"\"\"\n",
        "    h, w, _ = frame.shape\n",
        "    left_crop = int(w * 0.2)\n",
        "    right_crop = int(w * 0.8)\n",
        "\n",
        "    # Crop the horizontal sides\n",
        "    cropped_frame = frame[:, left_crop:right_crop]\n",
        "    rgb_frame = cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Run YOLOv8 detection\n",
        "    results = model.predict(rgb_frame, device=0, verbose=False)[0]\n",
        "\n",
        "    detected_boxes = []\n",
        "    for box in results.boxes:\n",
        "        cls_id = int(box.cls[0])\n",
        "        cls_name = model.names[cls_id]\n",
        "        if cls_name == 'person':\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            # Adjust x-coordinates back to original frame\n",
        "            x1 += left_crop\n",
        "            x2 += left_crop\n",
        "\n",
        "            #Draw bounding box on the original frame\n",
        "            #cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            \n",
        "            \n",
        "            detected_boxes.append((x1, y1, x2, y2, cls_name))\n",
        "\n",
        "    return frame, detected_boxes\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXBQouBfY75A"
      },
      "outputs": [],
      "source": [
        "# prompt: pull each video from the video output folder, and go through each one in a while loop, converting it to a numpy array\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "def ssim_diff(frame1, frame2):\n",
        "    grayA = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
        "    grayB = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
        "    score, _ = ssim(grayA, grayB, full=True)\n",
        "    return score\n",
        "\n",
        "#create a function to go through the video numpy array frame by frame and identify the gradient between each frame by a percentage\n",
        "def gradient_analysis(frames_array):\n",
        "    grad_arr = []\n",
        "    for i in range(1, len(frames_array)):\n",
        "        #calculate the gradient between the current frame and the previous frame\n",
        "        grad = ssim_diff(cv2.resize(frames_array[i], (100,100)), cv2.resize(frames_array[i-1], (100,100)))\n",
        "        grad_arr.append(grad)\n",
        "        #plot the grad_arr\n",
        "    # plt.plot(grad_arr)\n",
        "    # plt.show()\n",
        "    return grad_arr\n",
        "#given the grad_arr, we want to crop out the parts in frames array before or after the value in grad_arr goes below 0.4, keep the middle of the video\n",
        "def crop_video(frames_array, grad_arr):\n",
        "    if frames_array is None or len(frames_array) <= 10:\n",
        "        return [frames_array]\n",
        "    for i in range(int(len(grad_arr))):\n",
        "        if grad_arr[i] < 0.4:\n",
        "            #return the frames array before i and run crop_video on the frames array after i+2\n",
        "            return [frames_array[:i]] + crop_video(frames_array[i+2:], grad_arr[i+2:])\n",
        "    #if we reach here, it means we didn't find any frames to crop, so we return the original frames_array\n",
        "    return [frames_array]\n",
        "    \n",
        "            \n",
        "    return [frames_array]\n",
        "#after we crop, convert video to mp4 and save\n",
        "def convert_video(frames_array, output_path):\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, 20.0, (frames_array.shape[2], frames_array.shape[1]))\n",
        "    for frame in frames_array:\n",
        "        out.write(frame)\n",
        "    out.release()\n",
        "    return output_path\n",
        "\n",
        "#create a function to use MediaPipe Pose from google to identify the bounding box of all the players\n",
        "def detect_pitcher_bbox(video_path, max_frames=100):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    bboxes = []\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose = mp_pose.Pose()\n",
        "    for i in range(max_frames):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        h, w, _ = frame.shape\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def process_videos_in_folder(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Iterates through each MP4 video in the input folder, processes it to find the lowest human,\n",
        "    crops the video around that human, and saves the result in the output folder.\n",
        "\n",
        "    Args:\n",
        "        input_folder (str): The path to the folder containing input videos.\n",
        "        output_folder (str): The path to the folder where processed videos will be saved.\n",
        "    \"\"\"\n",
        "    if not os.path.isdir(input_folder):\n",
        "        print(f\"Error: Input folder not found at {input_folder}\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    video_files = [f for f in os.listdir(input_folder) if f.endswith('.mp4')]\n",
        "\n",
        "    if not video_files:\n",
        "        print(f\"No MP4 videos found in {input_folder}\")\n",
        "        return\n",
        "\n",
        "    for video_file in video_files:\n",
        "        video_path = os.path.join(input_folder, video_file)\n",
        "        print(f\"Processing video: {video_path}\")\n",
        "\n",
        "        frames_array = video_to_numpy(video_path)\n",
        "\n",
        "        gradient_array = gradient_analysis(frames_array)\n",
        "        print(f\"Gradient array shape: {np.array(gradient_array).shape}\")\n",
        "        print(f\"Gradient array: {gradient_array}\")\n",
        "\n",
        "        cropped_frames = crop_video(frames_array, gradient_array)\n",
        "\n",
        "        tracked_boxes = []\n",
        "\n",
        "    for i in range(len(cropped_frames)):\n",
        "        print(f\"Cropped frames array shape: {np.array(cropped_frames[i]).shape}\")\n",
        "\n",
        "        frames_segment = cropped_frames[i]\n",
        "        num_frames = len(frames_segment)\n",
        "        if num_frames == 0:\n",
        "            continue\n",
        "\n",
        "        bounding_boxes = []\n",
        "        annotated_frames = []\n",
        "\n",
        "        # Use the middle frame to choose which person to track\n",
        "        mid_index = num_frames // 2\n",
        "        mid_frame = frames_segment[mid_index]\n",
        "\n",
        "        # Detect all humans in the middle frame\n",
        "        mid_annotated_frame, mid_detected_boxes = detect_and_draw_all_humans(mid_frame)\n",
        "        candidates = [\n",
        "            (int(x1), int(y1), int(x2), int(y2))\n",
        "            for (x1, y1, x2, y2, cls) in mid_detected_boxes\n",
        "            if cls == 'person'\n",
        "        ]\n",
        "\n",
        "        if not candidates:\n",
        "            print(\"No humans detected in the middle frame. Skipping segment.\")\n",
        "            continue\n",
        "\n",
        "        # Pick the tallest person in the middle frame (as an example heuristic)\n",
        "        def height(b): return b[3] - b[1]\n",
        "        selected_box = max(candidates, key=height)\n",
        "\n",
        "        # Apply IoU to track this person across the segment\n",
        "        def iou(b1, b2):\n",
        "            xa, ya = max(b1[0], b2[0]), max(b1[1], b2[1])\n",
        "            xb, yb = min(b1[2], b2[2]), min(b1[3], b2[3])\n",
        "            inter_area = max(0, xb - xa) * max(0, yb - ya)\n",
        "            area1 = (b1[2] - b1[0]) * (b1[3] - b1[1])\n",
        "            area2 = (b2[2] - b2[0]) * (b2[3] - b2[1])\n",
        "            union_area = area1 + area2 - inter_area\n",
        "            return inter_area / union_area if union_area > 0 else 0\n",
        "\n",
        "        for frame in frames_segment:\n",
        "            annotated_frame, detected_boxes = detect_and_draw_all_humans(frame)\n",
        "            candidates = [\n",
        "                (int(x1), int(y1), int(x2), int(y2))\n",
        "                for (x1, y1, x2, y2, cls) in detected_boxes\n",
        "                if cls == 'person'\n",
        "            ]\n",
        "\n",
        "            best_box = max(candidates, key=lambda b: iou(b, selected_box), default=None) if candidates else None\n",
        "\n",
        "            # Check if match is good enough\n",
        "            if best_box and iou(best_box, selected_box) >= 0.5:\n",
        "                bounding_boxes.append(best_box)\n",
        "            else:\n",
        "                bounding_boxes.append(selected_box)  # fallback to selected\n",
        "            annotated_frames.append(annotated_frame)\n",
        "\n",
        "        if not bounding_boxes:\n",
        "            print(\"No valid humans detected in this segment. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        x1s, y1s, x2s, y2s = zip(*bounding_boxes)\n",
        "        crop_x1, crop_y1 = max(0, min(x1s)), max(0, min(y1s))\n",
        "        crop_x2, crop_y2 = min(frame.shape[1], max(x2s)), min(frame.shape[0], max(y2s))\n",
        "\n",
        "        if crop_y2 - crop_y1 > 550:\n",
        "            print(\"Cropped region too tall. Skipping segment.\")\n",
        "            continue\n",
        "\n",
        "        final_frames = [\n",
        "            frame[crop_y1:crop_y2, crop_x1:crop_x2] for frame in annotated_frames\n",
        "        ]\n",
        "\n",
        "        output_filename = f\"cropped_{i}_{video_file}\"\n",
        "        output_path = os.path.join(output_folder, output_filename)\n",
        "        result_path = convert_video(np.array(final_frames), output_path)\n",
        "\n",
        "        print(f\"Cropped video saved to: {result_path}\")\n",
        "\n",
        "\n",
        "\n",
        "        # Cleanup\n",
        "        try:\n",
        "            del frames_array\n",
        "        except NameError as e:\n",
        "            print(f\"NameError: {e}\")\n",
        "        try:\n",
        "            del gradient_array\n",
        "        except NameError as e:\n",
        "            print(f\"NameError: {e}\")\n",
        "        try:\n",
        "            del cropped_frames\n",
        "        except NameError as e:\n",
        "            print(f\"NameError: {e}\")\n",
        "        try:\n",
        "            del final_frames\n",
        "        except NameError as e:\n",
        "            print(f\"NameError: {e}\")\n",
        "        try:\n",
        "            del annotated_frames\n",
        "        except NameError as e:\n",
        "            print(f\"NameError: {e}\")\n",
        "        try:\n",
        "            del bounding_boxes\n",
        "        except NameError as e:\n",
        "            print(f\"NameError: {e}\")\n",
        "        try:\n",
        "            del result_path\n",
        "        except NameError as e:\n",
        "            print(f\"NameError: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def average_sampled_frames_from_array(frame_array, num_samples=10):\n",
        "    \"\"\"\n",
        "    Averages sampled frames from a NumPy array of frames.\n",
        "\n",
        "    Args:\n",
        "        frame_array (np.ndarray): A NumPy array of shape (num_frames, height, width, channels).\n",
        "        num_samples (int): Number of frames to sample and average.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The averaged frame resized to (100, 100).\n",
        "    \"\"\"\n",
        "    total_frames = frame_array.shape[0]\n",
        "    if total_frames < num_samples:\n",
        "        raise ValueError(\"Array has fewer frames than the number of samples requested.\")\n",
        "\n",
        "    indices = np.linspace(0, total_frames - 1, num_samples, dtype=int)\n",
        "    sampled_frames = frame_array[indices].astype(np.float32)\n",
        "\n",
        "    avg_frame = np.mean(sampled_frames, axis=0)\n",
        "    avg_frame = avg_frame.astype(np.uint8)\n",
        "\n",
        "    # Resize to 100x100\n",
        "    avg_frame = cv2.resize(avg_frame, (100, 100), interpolation=cv2.INTER_AREA)\n",
        "    return avg_frame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 25 * 25, 64)\n",
        "        self.fc2 = nn.Linear(64, 2)  # 2 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # -> (B, 16, 50, 50)\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # -> (B, 32, 25, 25)\n",
        "        x = x.view(-1, 32 * 25 * 25)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load the trained model\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# Re-create the model instance\n",
        "class_model = SimpleCNN()\n",
        "\n",
        "# Load state dict\n",
        "class_model.load_state_dict(torch.load(\"cnn_model.pth\", map_location=device))\n",
        "\n",
        "# Move model to GPU (if available)\n",
        "class_model = class_model.to(device)\n",
        "\n",
        "# Set model to evaluation mode if you're doing inference\n",
        "class_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "import shutil\n",
        "\n",
        "def process_numpy_video_for_inference(frame_array, class_model, video_name, num_samples=10, device='cuda'):\n",
        "    \"\"\"\n",
        "    Processes a NumPy array of video frames for inference using the given model.\n",
        "\n",
        "    Args:\n",
        "        frame_array (np.ndarray): NumPy array of shape (num_frames, height, width, channels), in RGB format.\n",
        "        model (torch.nn.Module): Trained PyTorch model for inference.\n",
        "        video_name (str): The name of the original video (used for saving output).\n",
        "        output_folder (str): Path to the output folder where results will be saved.\n",
        "        num_samples (int): Number of frames to sample and average.\n",
        "        device (str): 'cuda' or 'cpu'\n",
        "\n",
        "    Returns:\n",
        "        str: Predicted label ('pitching' or 'other').\n",
        "    \"\"\"\n",
        "    try:\n",
        "\n",
        "        # Compute average frame\n",
        "        avg_frame = average_sampled_frames_from_array(frame_array, num_samples)\n",
        "        avg_frame_tensor = torch.tensor(avg_frame).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
        "        avg_frame_tensor = avg_frame_tensor.to(device)\n",
        "\n",
        "        # Predict\n",
        "        class_model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = class_model(avg_frame_tensor)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "\n",
        "        label = 'pitching' if predicted.item() == 1 else 'other'\n",
        "\n",
        "\n",
        "        print(f\"Processed {video_name}: {label}\")\n",
        "        return label\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {video_name}: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J5TG_28lh42R",
        "outputId": "740122d1-f22c-4bac-aa5a-67cfd3729286"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "# Update output folder path\n",
        "output_folder = \"Video Outputs/\"\n",
        "\n",
        "\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "def crop_video_size(frames_array, detect_and_draw_fn):\n",
        "    \"\"\"\n",
        "    Processes a video in numpy array format, detects the lowest human in each frame,\n",
        "    crops all frames to the bounding box covering all detected humans, and returns\n",
        "    the cropped video as a numpy array.\n",
        "\n",
        "    Args:\n",
        "        frames_array (np.ndarray): Input video as a numpy array (frames, H, W, C).\n",
        "        detect_and_draw_fn (function): Function to detect and return (frame, bbox) for each frame.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Cropped video as a numpy array.\n",
        "    \"\"\"\n",
        "    bounding_boxes = []\n",
        "    annotated_frames = []\n",
        "\n",
        "    for frame in frames_array:\n",
        "        annotated_frame, bbox = detect_and_draw_fn(frame)\n",
        "        if bbox:\n",
        "            bounding_boxes.append(bbox)\n",
        "            annotated_frames.append(annotated_frame)\n",
        "\n",
        "    if not bounding_boxes:\n",
        "        print(\"No humans detected in this video. Returning original frames.\")\n",
        "        return frames_array\n",
        "\n",
        "    x1s, y1s, x2s, y2s = zip(*bounding_boxes)\n",
        "    crop_x1, crop_y1 = max(0, min(x1s)), max(0, min(y1s))\n",
        "    crop_x2, crop_y2 = min(frames_array.shape[2], max(x2s)), min(frames_array.shape[1], max(y2s))\n",
        "\n",
        "    final_frames = [\n",
        "        frame[crop_y1:crop_y2, crop_x1:crop_x2] for frame in annotated_frames\n",
        "    ]\n",
        "\n",
        "    return np.array(final_frames)\n",
        "\n",
        "import os\n",
        "import re\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#Function to flip the video frames horizontally\n",
        "def flip_video_frames(frames_array):\n",
        "    \"\"\"\n",
        "    Flips the video frames horizontally.\n",
        "\n",
        "    Args:\n",
        "        frames_array (np.ndarray): Input video as a numpy array (frames, H, W, C).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Flipped video as a numpy array.\n",
        "    \"\"\"\n",
        "    return np.flip(frames_array, axis=2)  # Flip along the width dimension\n",
        "\n",
        "def process_videos_from_df_numpy(df, starting_row, n_rows, output_folder):\n",
        "    \"\"\"\n",
        "    Processes the first n_rows of the DataFrame by downloading videos from their URLs,\n",
        "    detecting the main human subject, cropping the video accordingly, and saving the result.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame with a 'video_link' column.\n",
        "        n_rows (int): The number of rows to process.\n",
        "        output_folder (str): Where to save cropped videos.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    for index, row in df.head(n_rows).iterrows():\n",
        "        pitcher_name = row.get('pitcher')\n",
        "        #remove all non-alphanumeric characters from pitcher name\n",
        "        if pitcher_name:\n",
        "            pitcher_name = re.sub(r'\\W+', '', pitcher_name)\n",
        "        else:\n",
        "            pitcher_name = \"UnknownPitcher\"\n",
        "\n",
        "\n",
        "        # Create a subfolder for each pitcher\n",
        "        pitcher_folder = os.path.join(output_folder, pitcher_name)\n",
        "        os.makedirs(pitcher_folder, exist_ok=True)\n",
        "        if index < starting_row:\n",
        "            continue\n",
        "        # Extract video link\n",
        "        video_page_url = row.get('video_link')\n",
        "        if pd.isna(video_page_url) or not video_page_url:\n",
        "            print(f\"Skipping row {index} due to missing video_link.\")\n",
        "            continue\n",
        "\n",
        "        match = re.search(r\"playId=([a-z0-9\\-]+)\", video_page_url)\n",
        "        if not match:\n",
        "            print(f\"Could not extract playId from URL: {video_page_url}\")\n",
        "            continue\n",
        "\n",
        "        play_id = match.group(1)\n",
        "        video_url = get_video_url(play_id)\n",
        "\n",
        "        if not video_url:\n",
        "            print(f\"No video URL found for playId: {play_id}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing row index: {index}, play_id: {play_id}\")\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as tmp:\n",
        "            download_video(video_url, tmp.name)\n",
        "            temp_video_path = tmp.name\n",
        "\n",
        "        try:\n",
        "            frames_array = video_to_numpy(temp_video_path)\n",
        "            gradient_array = gradient_analysis(frames_array)\n",
        "            cropped_segments = crop_video(frames_array, gradient_array)\n",
        "\n",
        "\n",
        "\n",
        "            for k, segment in enumerate(cropped_segments):\n",
        "                try:\n",
        "                    #process numpy video for inference\n",
        "                    label = process_numpy_video_for_inference(segment, class_model, play_id, num_samples=10, device='cuda')\n",
        "                    if label != 'pitching':\n",
        "                        print(f\"Skipping segment {k} for play_id {play_id} as it is not a pitching video.\")\n",
        "                        continue\n",
        "                    # Choose the middle frame of the full video to identify the person\n",
        "                    mid_index = len(segment) // 2\n",
        "                    mid_frame = segment[mid_index]\n",
        "\n",
        "                    mid_annotated, mid_boxes = detect_and_draw_all_humans(mid_frame)\n",
        "                    candidates = [\n",
        "                        (int(x1), int(y1), int(x2), int(y2))\n",
        "                        for (x1, y1, x2, y2, cls) in mid_boxes if cls == 'person'\n",
        "                    ]\n",
        "                    #remove all candidates where the height, width is less than 75 pixels\n",
        "                    candidates = [b for b in candidates if (b[3] - b[1]) > 75]\n",
        "                    candidates = [b for b in candidates if (b[2] - b[0]) > 75]\n",
        "\n",
        "\n",
        "                    if not candidates:\n",
        "                        print(\"No humans found in the middle frame. Skipping video.\")\n",
        "                        continue\n",
        "\n",
        "                    # Select most central person based on midpoint\n",
        "                    def midpoint(b):\n",
        "                        return (b[0] + b[2]) / 2, (b[1] + b[3]) / 2\n",
        "                    mid_points = [midpoint(b) for b in candidates]\n",
        "                    # Set mid_y as 40% of the height of the middle frame\n",
        "                    mid_y = int(mid_frame.shape[0] * 0.65)\n",
        "                    # Select the box whose midpoint y is closest to mid_y\n",
        "                    selected_box = min(candidates, key=lambda b: abs(midpoint(b)[1] - mid_y))\n",
        "                    print(f\"Selected box for tracking: {selected_box}\")\n",
        "                    selected_box = (int(selected_box[0]), int(selected_box[1]), int(selected_box[2]), int(selected_box[3]))\n",
        "\n",
        "                    # Track the selected person across the entire video using IoU\n",
        "                    def iou(b1, b2):\n",
        "                        xa, ya = max(b1[0], b2[0]), max(b1[1], b2[1])\n",
        "                        xb, yb = min(b1[2], b2[2]), min(b1[3], b2[3])\n",
        "                        inter_area = max(0, xb - xa) * max(0, yb - ya)\n",
        "                        area1 = (b1[2] - b1[0]) * (b1[3] - b1[1])\n",
        "                        area2 = (b2[2] - b2[0]) * (b2[3] - b2[1])\n",
        "                        union_area = area1 + area2 - inter_area\n",
        "                        return inter_area / union_area if union_area > 0 else 0\n",
        "\n",
        "                    tracked_boxes = [None] * len(segment)\n",
        "                    tracked_boxes[mid_index] = selected_box\n",
        "                    annotated_frames = [None] * len(segment)\n",
        "\n",
        "                    # Track forward from middle\n",
        "                    prev_box = selected_box\n",
        "                    for i in range(mid_index, len(segment)):\n",
        "                        frame = segment[i]\n",
        "                        annotated, detected_boxes = detect_and_draw_all_humans(frame)\n",
        "                        annotated_frames[i] = annotated\n",
        "\n",
        "                        candidates = [\n",
        "                            (int(x1), int(y1), int(x2), int(y2))\n",
        "                            for (x1, y1, x2, y2, cls) in detected_boxes if cls == 'person'\n",
        "                        ]\n",
        "\n",
        "                        best_box = max(candidates, key=lambda b: iou(b, prev_box), default=None) if candidates else None\n",
        "                        if best_box and iou(best_box, prev_box) >= 0.75:\n",
        "                            tracked_boxes[i] = best_box\n",
        "                            prev_box = best_box\n",
        "                        else:\n",
        "                            tracked_boxes[i] = prev_box  # fallback\n",
        "\n",
        "                    # Track backward from middle\n",
        "                    prev_box = selected_box\n",
        "                    for i in range(mid_index - 1, -1, -1):\n",
        "                        frame = segment[i]\n",
        "                        annotated, detected_boxes = detect_and_draw_all_humans(frame)\n",
        "                        annotated_frames[i] = annotated\n",
        "\n",
        "                        candidates = [\n",
        "                            (int(x1), int(y1), int(x2), int(y2))\n",
        "                            for (x1, y1, x2, y2, cls) in detected_boxes if cls == 'person'\n",
        "                        ]\n",
        "\n",
        "                        best_box = max(candidates, key=lambda b: iou(b, prev_box), default=None) if candidates else None\n",
        "                        if best_box and iou(best_box, prev_box) >= 0.75:\n",
        "                            tracked_boxes[i] = best_box\n",
        "                            prev_box = best_box\n",
        "                        else:\n",
        "                            tracked_boxes[i] = prev_box\n",
        "\n",
        "                    # Compute crop box from all tracked positions\n",
        "                    x1s, y1s, x2s, y2s = zip(*tracked_boxes)\n",
        "                    crop_x1, crop_y1 = max(0, min(x1s)), max(0, min(y1s))\n",
        "                    crop_x2, crop_y2 = min(frame.shape[1], max(x2s)), min(frame.shape[0], max(y2s))\n",
        "\n",
        "                    # Add 10% padding\n",
        "                    padding_x = int(0.1 * (crop_x2 - crop_x1))\n",
        "                    padding_y = int(0.05 * (crop_y2 - crop_y1))\n",
        "                    crop_x1 = max(0, crop_x1 - padding_x)\n",
        "                    crop_y1 = max(0, crop_y1 - padding_y)\n",
        "                    crop_x2 = min(frame.shape[1], crop_x2 + padding_x)\n",
        "                    crop_y2 = min(frame.shape[0], crop_y2 + padding_y)\n",
        "                    #update the crop box to make it a square\n",
        "                    max_dim = max(crop_y2 - crop_y1, crop_x2 - crop_x1)\n",
        "                    if (crop_y2 - crop_y1) < max_dim:\n",
        "                        crop_y1 = (crop_y2 + crop_y1 - max_dim)//2\n",
        "                        crop_y2 = (crop_y1 + max_dim + crop_y2)//2\n",
        "\n",
        "                    if (crop_x2 - crop_x1) < max_dim:\n",
        "                        crop_x1 = (crop_x2 + crop_x1 - max_dim)//2\n",
        "                        crop_x2 = (crop_x1 + max_dim + crop_x2)//2\n",
        "\n",
        "                \n",
        "\n",
        "                    #if height or width of the crop is less than 75 pixels, skip this segment\n",
        "                    if (crop_y2 - crop_y1) < 100 or (crop_x2 - crop_x1) < 100:\n",
        "                        print(f\"Skipping segment {k} for play_id {play_id} due to small crop size.\")\n",
        "                        continue\n",
        "\n",
        "                    # Final crop\n",
        "                    final_frames = [\n",
        "                        frame[crop_y1:crop_y2, crop_x1:crop_x2] for frame in annotated_frames\n",
        "                    ]\n",
        "\n",
        "                    # If the last character is 'L', flip the video frames horizontally\n",
        "                    if pitcher_name.endswith('L'):\n",
        "                        print(f\"Flipping video frames for left-handed pitcher: {pitcher_name}\")\n",
        "                        final_frames = flip_video_frames(final_frames)\n",
        "\n",
        "                    #Add square padding to the final frames\n",
        "                    # max_dim = max(crop_y2 - crop_y1, crop_x2 - crop_x1)\n",
        "                    # padded_frames = []\n",
        "                    # for frame in final_frames:\n",
        "                    #     h, w, _ = frame.shape\n",
        "                    #     if h < max_dim:\n",
        "                    #         pad_h = (max_dim - h) // 2\n",
        "                    #         frame = cv2.copyMakeBorder(frame, pad_h, pad_h, 0, 0, cv2.BORDER_CONSTANT)\n",
        "                    #     if w < max_dim:\n",
        "                    #         pad_w = (max_dim - w) // 2\n",
        "                    #         frame = cv2.copyMakeBorder(frame, 0, 0, pad_w, pad_w, cv2.BORDER_CONSTANT)\n",
        "                    #     padded_frames.append(frame)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    output_filename = f\"cropped_{play_id}_{k}.mp4\"\n",
        "                    output_path = os.path.join(pitcher_folder, output_filename)\n",
        "                    result_path = convert_video(np.array(final_frames), output_path)\n",
        "\n",
        "                    print(f\"Saved cropped video to: {result_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing segment {k} for play_id {play_id}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        finally:\n",
        "            os.remove(temp_video_path)\n",
        "\n",
        "\n",
        "# Example usage: Process the first 5 rows and save to 'Video Outputs' folder\n",
        "#process_videos_from_df_numpy(df, 0, 1300, output_folder+f\"cropping_videos/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3\n",
        "\n",
        "def upload_to_s3(file_path, object_name, bucket_name='shadow-trainer-dev',\n",
        "                 aws_access_key_id='',\n",
        "                 aws_secret_access_key='',\n",
        "                 region_name='us-east-2'):\n",
        "    \"\"\"\n",
        "    Uploads a file to the specified S3 bucket.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the local file to upload.\n",
        "        object_name (str): Path in the S3 bucket (including the file name).\n",
        "        bucket_name (str): Name of the S3 bucket. Default is 'shadow-trainer-dev'.\n",
        "        aws_access_key_id (str): AWS access key ID.\n",
        "        aws_secret_access_key (str): AWS secret access key.\n",
        "        region_name (str): AWS region.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if upload succeeded, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        s3_client = boto3.client(\n",
        "            's3',\n",
        "            aws_access_key_id=aws_access_key_id,\n",
        "            aws_secret_access_key=aws_secret_access_key,\n",
        "            region_name=region_name\n",
        "        )\n",
        "\n",
        "        s3_client.upload_file(file_path, bucket_name, object_name)\n",
        "        print(f\"Successfully uploaded {file_path} to s3://{bucket_name}/{object_name}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading file: {e}\")\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_recovery_point(rankle_y_diff):\n",
        "    #Return the recovery point, which is the index of the first occurrence of rankle_y_diff > 0.5\n",
        "    recovery_point = -1\n",
        "    for i in range(len(rankle_y_diff)):\n",
        "        if rankle_y_diff[i] > 0.5:\n",
        "            recovery_point = i\n",
        "            break\n",
        "    return recovery_point\n",
        "\n",
        "def get_keypoints(keypoints):\n",
        "    #get the save thre right wrist, right ankle and left ankle in an array of shape (3, 3) with the x, y, and confidence\n",
        "    if keypoints is None:\n",
        "        return np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
        "    keypoints = keypoints.cpu().numpy()\n",
        "    min_leg = 100000\n",
        "    count = 0\n",
        "    for joints in keypoints.xy:\n",
        "        count += 1\n",
        "        # print(f\"Processing keypoint {count}: {joints}\")\n",
        "        if joints[16][1] < min_leg:\n",
        "            min_leg = joints[16][1]\n",
        "            right_wrist = joints[10]\n",
        "            right_ankle = joints[15]\n",
        "            left_ankle = joints[16]\n",
        "    # print(f\"Right Wrist: {right_wrist}, Right Ankle: {right_ankle}, Left Ankle: {left_ankle}\")\n",
        "    return np.array([right_wrist, right_ankle, left_ankle])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the YOLO11s-pose model and move to CUDA if available\n",
        "model = YOLO('yolo11x-pose.pt')\n",
        "if hasattr(model, 'to'):\n",
        "    model.to('cuda')\n",
        "\n",
        "def pose_and_overlay_video(input_video_path):\n",
        "    points_by_frame = []\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        results = model(frame, device=0)  # Force CUDA (device=0)\n",
        "        #get the pose points\n",
        "        #print(results)\n",
        "        points_for_frame = []\n",
        "        max_leg = -1\n",
        "        for result in results:\n",
        "            if result.keypoints is not None:\n",
        "                for keypoint in result.keypoints:\n",
        "                    if keypoint is not None:\n",
        "                        keypoints_extracted = get_keypoints(keypoint)\n",
        "                        if keypoints_extracted[2][1] > max_leg:\n",
        "                            max_leg = keypoints_extracted[2][1]\n",
        "                            points_for_frame = keypoints_extracted\n",
        "        points_by_frame.append(points_for_frame)\n",
        "        # print(f\"Frame {len(points_by_frame)}: {points_for_frame}\")\n",
        "        for point in points_by_frame:\n",
        "            focus_point = point[0]\n",
        "            if focus_point[0] > 0.0:\n",
        "                cv2.circle(frame, (int(focus_point[0]), int(focus_point[1])), 2, (0, 255, 0), -1)\n",
        "\n",
        "            focus_point = point[1]\n",
        "            if focus_point[0] > 0.0:\n",
        "                cv2.circle(frame, (int(focus_point[0]), int(focus_point[1])), 2, (255, 0, 0), -1)\n",
        "\n",
        "            focus_point = point[2]\n",
        "            if focus_point[0] > 0.0:\n",
        "                cv2.circle(frame, (int(focus_point[0]), int(focus_point[1])), 2, (0, 0, 255), -1)\n",
        "        # Display the frame with pose points\n",
        "        annotated_frame = frame.copy()\n",
        "        #annotated_frame = results[0].plot()\n",
        "        # Display the annotated frame\n",
        "        # cv2.imshow(\"Pose Overlay\", annotated_frame)\n",
        "        # if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "        #     break\n",
        "\n",
        "    # get the difference between the current frame and the previous frame for the right wrist and plot it\n",
        "    diff = [0]\n",
        "    lankle_y = [0]\n",
        "    rankle_y = [0]\n",
        "    #smoothen each joint x and y position\n",
        "    points_by_frame = np.array(points_by_frame)\n",
        "    print(points_by_frame.shape)\n",
        "    points_by_frame = np.nan_to_num(points_by_frame, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    #for each frame, if val is 0.0, replace it with the previous value\n",
        "    for i in range(1, len(points_by_frame)):\n",
        "        for j in range(3):  # Assuming 3 keypoints: right wrist, right ankle, left ankle\n",
        "            if points_by_frame[i][j][0] == 0.0:\n",
        "                points_by_frame[i][j] = points_by_frame[i-1][j]\n",
        "    points_by_frame = points_by_frame[:, :, :2]  # Keep only x and y coordinates\n",
        "    points_by_frame = np.apply_along_axis(lambda x: np.convolve(x, np.ones(5)/5, mode='valid'), axis=0, arr=points_by_frame)\n",
        "    print(points_by_frame.shape)\n",
        "    #points_by_frame = points_by_frame.tolist()  # Convert back to list for iteration\n",
        "    for i in range(1, len(points_by_frame)):\n",
        "        right_wrist_current = points_by_frame[i][0]\n",
        "        right_wrist_previous = points_by_frame[i-1][0]\n",
        "        left_ankle_current = points_by_frame[i][1]\n",
        "        left_ankle_previous = points_by_frame[i-1][1]\n",
        "        right_ankle_current = points_by_frame[i][2]\n",
        "        right_ankle_previous = points_by_frame[i-1][2]\n",
        "        if left_ankle_current[0] > 0.0 and left_ankle_previous[0] > 0.0:\n",
        "            difference = np.linalg.norm(left_ankle_current[:2] - left_ankle_previous[:2])\n",
        "            print(f\"Difference in left ankle position between frame {i} and {i-1}: {difference}\")\n",
        "            lankle_y.append(left_ankle_current[1])\n",
        "        else:\n",
        "            lankle_y.append(lankle_y[-1])\n",
        "        if right_ankle_current[0] > 0.0 and right_ankle_previous[0] > 0.0:\n",
        "            difference = np.linalg.norm(right_ankle_current[:2] - right_ankle_previous[:2])\n",
        "            print(f\"Difference in right ankle position between frame {i} and {i-1}: {difference}\")\n",
        "            rankle_y.append(right_ankle_current[1])\n",
        "        else:\n",
        "            rankle_y.append(rankle_y[-1])\n",
        "        if right_wrist_current[0] > 0.0 and right_wrist_previous[0] > 0.0:\n",
        "            difference = np.linalg.norm(right_wrist_current[:2] - right_wrist_previous[:2])\n",
        "            print(f\"Difference in right wrist position between frame {i} and {i-1}: {difference}\")\n",
        "            diff.append(difference)\n",
        "        else:\n",
        "            diff.append(diff[-1])\n",
        "    #smooth diff, lankle_y, and rankle_y\n",
        "    diff = np.convolve(diff[1:], np.ones(10)/10, mode='valid')\n",
        "    # lankle_y = np.convolve(lankle_y, np.ones(5)/5, mode='valid')\n",
        "    # rankle_y = np.convolve(rankle_y, np.ones(5)/5, mode='valid')\n",
        "    #normalize the diff, lankle_y, and rankle_y\n",
        "    diff = (diff - np.min(diff)) / (np.max(diff) - np.min(diff))\n",
        "    #cap lankle_y and diff to range of 0 to 10\n",
        "    lankle_y = np.array(lankle_y)\n",
        "    rankle_y = np.array(rankle_y)\n",
        "    # lankle_y = np.clip(lankle_y, 0, 10)\n",
        "    # diff = np.clip(diff, 0, 10)\n",
        "    lankle_y = lankle_y[1:]  # Remove the first element to match the length of diff\n",
        "    rankle_y = rankle_y[1:]  # Remove the first element to match the length of diff \n",
        "    lankle_y = (lankle_y - np.min(lankle_y)) / (np.max(lankle_y) - np.min(lankle_y))\n",
        "    rankle_y = (rankle_y - np.min(rankle_y)) / (np.max(rankle_y) - np.min(rankle_y))\n",
        "    lankle_y_diff = np.diff(lankle_y, prepend=lankle_y[0])  # Prepend the first value to maintain the same length\n",
        "    rankle_y_diff = np.diff(rankle_y, prepend=rankle_y[0])  # Prepend the first value to maintain the same length\n",
        "    #smooth the differences\n",
        "    lankle_y_diff = np.convolve(lankle_y_diff, np.ones(10)/10, mode='valid')\n",
        "    rankle_y_diff = np.convolve(rankle_y_diff, np.ones(10)/10, mode='valid')\n",
        "    lankle_y_diff = np.convolve(lankle_y_diff, np.ones(5)/5, mode='valid')\n",
        "    rankle_y_diff = np.convolve(rankle_y_diff, np.ones(5)/5, mode='valid')\n",
        "\n",
        "    lankle_y_diff = np.where(lankle_y_diff < 0.01, lankle_y_diff, .75)\n",
        "    rankle_y_diff = np.where(rankle_y_diff < 0.02, rankle_y_diff, 1)\n",
        "    lankle_y_diff = np.where(lankle_y_diff > -0.01, lankle_y_diff, -.75)\n",
        "    rankle_y_diff = np.where(rankle_y_diff > -0.02, rankle_y_diff, -1)\n",
        "    #identify all frames where it goes from less than -0.5 to positive 0.5 within 10 frames\n",
        "    lankle_change = []\n",
        "    for i in range(len(lankle_y_diff) - 20):\n",
        "        if lankle_y_diff[i] < -0.5 and np.any(lankle_y_diff[i:i+20] > 0.5):\n",
        "            #if the prev value in lankle_change is within 5 frames, replace it\n",
        "            if lankle_change and i - lankle_change[-1] < 5:\n",
        "                lankle_change[-1] = i\n",
        "            else:\n",
        "                lankle_change.append(i)\n",
        "    rankle_change = []\n",
        "    for i in range(len(rankle_y_diff) - 20):\n",
        "        if rankle_y_diff[i] < -0.5 and np.any(rankle_y_diff[i:i+20] > 0.5):\n",
        "            #if the prev value in lankle_change is within 5 frames, replace it\n",
        "            if rankle_change and i - rankle_change[-1] < 5:\n",
        "                rankle_change[-1] = i\n",
        "            else:\n",
        "                rankle_change.append(i)\n",
        "    print(f\"Left Ankle Recovery Points: {lankle_change}\")\n",
        "    print(f\"Right Ankle Recovery Points: {rankle_change}\")\n",
        "\n",
        "    # Go through each value in rankle_change, for each rankle_change, check if there is a lankle_change 40 to 95 frames before it, if so, print the lankle_change and the rankle_change\n",
        "    recovery = ()\n",
        "    for rankle in rankle_change:\n",
        "        for lankle in lankle_change:\n",
        "            if lankle < rankle and rankle - lankle > 22 and rankle - lankle < 95 and recovery == ():\n",
        "                recovery = (lankle, rankle)\n",
        "                #break from both for loops\n",
        "                break\n",
        "    if recovery != ():\n",
        "        #Go through lankle_y_diff from recovery[0] to the beginning\n",
        "        for i in range(recovery[0], -1, -1):\n",
        "            if lankle_y_diff[i] < -0.5 and recovery[0] - i < 10:\n",
        "                recovery = (i, recovery[1])\n",
        "        # Go through rankle_y_diff from recovery[1] to the end\n",
        "        for i in range(recovery[1]+10, len(rankle_y_diff)):\n",
        "            if rankle_y_diff[i] > 0.5 and i - recovery[1] < 20:\n",
        "                recovery = (recovery[0], i)\n",
        "        recovery = (max(recovery[0]-5, 0), min(recovery[1]+20, len(rankle_y_diff)))\n",
        "\n",
        "\n",
        "            \n",
        "    # Plot the differences\n",
        "            \n",
        "    print(recovery)\n",
        "    x = np.arange(len(rankle_y_diff))\n",
        "    # Plot the differences  \n",
        "\n",
        "    #plt.plot(diff, color='red', label='Right Wrist Position Difference')\n",
        "    plt.plot(lankle_y, label='Left Ankle Y Position')\n",
        "    plt.plot(rankle_y, label='Right Ankle Y Position', color='orange')\n",
        "    plt.scatter(x,lankle_y_diff, label='Left Ankle Y Position Difference', color='blue')\n",
        "    plt.scatter(x,rankle_y_diff, label='Right Ankle Y Position Difference', color='purple')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.title('Difference in Right Wrist Position Over Frames')\n",
        "    plt.xlabel('Frame Index')\n",
        "    plt.ylabel('Difference (pixels)')\n",
        "    plt.show()\n",
        "\n",
        "    #play the video from frame recovery[0] to recovery[1]\n",
        "    #cap.set(cv2.CAP_PROP_POS_FRAMES, recovery[0])\n",
        "    # Uncomment the following lines to visualize the video with pose overlay\n",
        "    # while cap.isOpened():\n",
        "    #     ret, frame = cap.read()\n",
        "    #     if not ret or cap.get(cv2.CAP_PROP_POS_FRAMES) > recovery[1]:\n",
        "    #         break\n",
        "    #     results = model(frame, device=0)  # Force CUDA (device=0)\n",
        "    #     points_for_frame = []\n",
        "    #     for result in results:\n",
        "    #         if result.keypoints is not None:\n",
        "    #             for keypoint in result.keypoints:\n",
        "    #                 if keypoint is not None:\n",
        "    #                     keypoints_extracted = get_keypoints(keypoint)\n",
        "    #                     points_for_frame.append(keypoints_extracted)\n",
        "    #     for point in points_for_frame:\n",
        "    #         focus_point = point[0]\n",
        "    #         if focus_point[0] > 0.0:\n",
        "    #             cv2.circle(frame, (int(focus_point[0]), int(focus_point[1])), 2, (0, 255, 0), -1)\n",
        "\n",
        "    #         focus_point = point[1]\n",
        "    #         if focus_point[0] > 0.0:\n",
        "    #             cv2.circle(frame, (int(focus_point[0]), int(focus_point[1])), 2, (255, 0, 0), -1)\n",
        "\n",
        "    #         focus_point = point[2]\n",
        "    #         if focus_point[0] > 0.0:\n",
        "    #             cv2.circle(frame, (int(focus_point[0]), int(focus_point[1])), 2, (0, 0, 255), -1)\n",
        "\n",
        "    #     cv2.imshow(\"Pose Overlay\", frame)\n",
        "    #     if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "    #         break\n",
        "\n",
        "\n",
        "        \n",
        "    \n",
        "        \n",
        "                    \n",
        "                        \n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "            #return recovery if it is not empty\n",
        "    if recovery == ():\n",
        "        print(\"No recovery points found.\")\n",
        "        return None\n",
        "    else:\n",
        "        return recovery\n",
        "\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# pose_and_overlay_video('input.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_single_mp4_video(video_path):\n",
        "    \"\"\"\n",
        "    Process a single mp4 video: detect recovery points, crop the video between those points,\n",
        "    and overwrite the original video with the cropped version.\n",
        "    \"\"\"\n",
        "    print(f\"Processing: {video_path}\")\n",
        "\n",
        "    try:\n",
        "        recovery = pose_and_overlay_video(video_path)\n",
        "        if recovery is not None:\n",
        "            start_frame, end_frame = recovery\n",
        "            print(f\"Recovery points found in {video_path}: start={start_frame}, end={end_frame}\")\n",
        "\n",
        "            # Load original video\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "            # Write to a temporary file first\n",
        "            temp_path = video_path + \".temp.mp4\"\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "            out = cv2.VideoWriter(temp_path, fourcc, fps, (width, height))\n",
        "\n",
        "            frame_idx = 0\n",
        "            while cap.isOpened():\n",
        "                ret, frame = cap.read()\n",
        "                if not ret or frame_idx > end_frame:\n",
        "                    break\n",
        "                if start_frame <= frame_idx <= end_frame:\n",
        "                    out.write(frame)\n",
        "                frame_idx += 1\n",
        "\n",
        "            cap.release()\n",
        "            out.release()\n",
        "\n",
        "            # Overwrite original video\n",
        "            import os\n",
        "            os.replace(temp_path, video_path)\n",
        "            print(f\"Overwritten original video with cropped segment at {video_path}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"No recovery points found in {video_path}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {video_path}: {e}\")\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Load in final_cnn_model.pth\n",
        "# Load the trained model\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# Re-create the model instance\n",
        "final_class_model = SimpleCNN()\n",
        "\n",
        "# Load state dict\n",
        "final_class_model.load_state_dict(torch.load(\"final_cnn_model.pth\", map_location=device))\n",
        "\n",
        "# Move model to GPU (if available)\n",
        "final_class_model = final_class_model.to(device)\n",
        "\n",
        "# Set model to evaluation mode if you're doing inference\n",
        "final_class_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_s3_object_exists(bucket_name, object_name, aws_access_key_id='',\n",
        "                           aws_secret_access_key='',\n",
        "                           region_name='us-east-2'):\n",
        "    \"\"\"\n",
        "    Check if an object exists in an S3 bucket.\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): Name of the S3 bucket.\n",
        "        object_name (str): Path to the object in the S3 bucket.\n",
        "        aws_access_key_id (str): AWS access key ID.\n",
        "        aws_secret_access_key (str): AWS secret access key.\n",
        "        region_name (str): AWS region.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the object exists, False otherwise.\n",
        "    \"\"\"\n",
        "    s3_client = boto3.client(\n",
        "        's3',\n",
        "        aws_access_key_id=aws_access_key_id,\n",
        "        aws_secret_access_key=aws_secret_access_key,\n",
        "        region_name=region_name\n",
        "    )\n",
        "    try:\n",
        "        s3_client.head_object(Bucket=bucket_name, Key=object_name)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking S3 object: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import tempfile\n",
        "import queue\n",
        "import threading\n",
        "import concurrent.futures\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def process_videos_from_df_numpy(df, starting_row, n_rows, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    task_queue = queue.Queue(maxsize=1)\n",
        "    stop_signal = object()\n",
        "\n",
        "    def producer():\n",
        "        print(f\"Starting producer thread for rows {starting_row} to {starting_row + n_rows}\")\n",
        "        for index, row in df.head(n_rows).iterrows():\n",
        "            print(f\"Processing row index: {index}, pitcher: {row.get('pitcher')}, video_link: {row.get('video_link')}\")\n",
        "            if index < starting_row:\n",
        "                continue\n",
        "\n",
        "            pitcher_name = row.get('pitcher')\n",
        "            pitcher_name = re.sub(r'\\W+', '', pitcher_name) if pitcher_name else \"UnknownPitcher\"\n",
        "            pitch_type = row.get('pitch', 'UnknownPitchType')\n",
        "            pitcher_folder = os.path.join(output_folder, pitcher_name)\n",
        "            os.makedirs(pitcher_folder, exist_ok=True)\n",
        "\n",
        "            video_page_url = row.get('video_link')\n",
        "            if pd.isna(video_page_url) or not video_page_url:\n",
        "                print(f\"Skipping row {index} due to missing video_link.\")\n",
        "                continue\n",
        "\n",
        "            match = re.search(r\"playId=([a-z0-9\\-]+)\", video_page_url)\n",
        "            if not match:\n",
        "                print(f\"Could not extract playId from URL: {video_page_url}\")\n",
        "                continue\n",
        "\n",
        "            play_id = match.group(1)\n",
        "            video_url = get_video_url(play_id)\n",
        "\n",
        "            if not video_url:\n",
        "                print(f\"No video URL found for playId: {play_id}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as tmp:\n",
        "                    download_video(video_url, tmp.name)\n",
        "                    temp_video_path = tmp.name\n",
        "\n",
        "                frames_array = video_to_numpy(temp_video_path)\n",
        "                os.remove(temp_video_path)\n",
        "                task_queue.put((index, row, frames_array, play_id, pitcher_folder, pitcher_name, pitch_type))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in producer for play_id {play_id}: {e}\")\n",
        "                continue\n",
        "\n",
        "        task_queue.put(stop_signal)\n",
        "\n",
        "    def consumer():\n",
        "        while True:\n",
        "            item = task_queue.get()\n",
        "            if item is stop_signal:\n",
        "                break\n",
        "\n",
        "            index, row, frames_array, play_id, pitcher_folder, pitcher_name, pitch_type = item\n",
        "            try:\n",
        "                gradient_array = gradient_analysis(frames_array) #find sharp changes in the video\n",
        "                cropped_segments = crop_video(frames_array, gradient_array) #identify camera changes based on shifts, split by segments\n",
        "\n",
        "                for k, segment in enumerate(cropped_segments):\n",
        "                    try:\n",
        "                        output_filename = f\"cropped_{play_id}_{k}.mp4\"\n",
        "                        s3_location = f\"cropped_videos/{pitcher_name}/{pitch_type}/{output_filename}\"\n",
        "                        #check if the video already exists in S3\n",
        "                        if check_s3_object_exists('shadow-trainer-dev' ,s3_location):\n",
        "                            print(f\"Video {output_filename} already exists in S3. Skipping.\")\n",
        "                            continue\n",
        "                        label = process_numpy_video_for_inference(segment, class_model, play_id, num_samples=10, device='cuda')#classify video segment as pitching or not\n",
        "                        if label != 'pitching':\n",
        "                            print(f\"Skipping segment {k} for play_id {play_id} as it is not a pitching video.\")\n",
        "                            continue\n",
        "\n",
        "                        mid_index = len(segment) // 2\n",
        "                        mid_frame = segment[mid_index]\n",
        "                        mid_annotated, mid_boxes = detect_and_draw_all_humans(mid_frame)\n",
        "                        candidates = [(int(x1), int(y1), int(x2), int(y2)) for (x1, y1, x2, y2, cls) in mid_boxes if cls == 'person']\n",
        "                        candidates = [b for b in candidates if (b[3] - b[1]) > 75 and (b[2] - b[0]) > 75]#Identify pitcher\n",
        "\n",
        "                        if not candidates:\n",
        "                            print(\"No humans found in the middle frame. Skipping video.\")\n",
        "                            continue\n",
        "\n",
        "                        def midpoint(b): return (b[0] + b[2]) / 2, (b[1] + b[3]) / 2\n",
        "                        mid_y = int(mid_frame.shape[0] * 0.65)\n",
        "                        selected_box = min(candidates, key=lambda b: abs(midpoint(b)[1] - mid_y))\n",
        "                        selected_box = tuple(map(int, selected_box))\n",
        "                        print(f\"Selected box for tracking: {selected_box}\")\n",
        "\n",
        "                        def iou(b1, b2):\n",
        "                            xa, ya = max(b1[0], b2[0]), max(b1[1], b2[1])\n",
        "                            xb, yb = min(b1[2], b2[2]), min(b1[3], b2[3])\n",
        "                            inter_area = max(0, xb - xa) * max(0, yb - ya)\n",
        "                            area1 = (b1[2] - b1[0]) * (b1[3] - b1[1])\n",
        "                            area2 = (b2[2] - b2[0]) * (b2[3] - b2[1])\n",
        "                            union_area = area1 + area2 - inter_area\n",
        "                            return inter_area / union_area if union_area > 0 else 0\n",
        "\n",
        "                        tracked_boxes = [None] * len(segment)\n",
        "                        tracked_boxes[mid_index] = selected_box\n",
        "                        annotated_frames = [None] * len(segment)\n",
        "\n",
        "                        prev_box = selected_box\n",
        "                        for i in range(mid_index, len(segment)):\n",
        "                            frame = segment[i]\n",
        "                            annotated, detected_boxes = detect_and_draw_all_humans(frame)\n",
        "                            annotated_frames[i] = annotated\n",
        "                            candidates = [(int(x1), int(y1), int(x2), int(y2)) for (x1, y1, x2, y2, cls) in detected_boxes if cls == 'person']\n",
        "                            best_box = max(candidates, key=lambda b: iou(b, prev_box), default=None) if candidates else None\n",
        "                            tracked_boxes[i] = best_box if best_box and iou(best_box, prev_box) >= 0.75 else prev_box\n",
        "                            prev_box = tracked_boxes[i]\n",
        "\n",
        "                        prev_box = selected_box\n",
        "                        for i in range(mid_index - 1, -1, -1):\n",
        "                            frame = segment[i]\n",
        "                            annotated, detected_boxes = detect_and_draw_all_humans(frame)\n",
        "                            annotated_frames[i] = annotated\n",
        "                            candidates = [(int(x1), int(y1), int(x2), int(y2)) for (x1, y1, x2, y2, cls) in detected_boxes if cls == 'person']\n",
        "                            best_box = max(candidates, key=lambda b: iou(b, prev_box), default=None) if candidates else None\n",
        "                            tracked_boxes[i] = best_box if best_box and iou(best_box, prev_box) >= 0.75 else prev_box\n",
        "                            prev_box = tracked_boxes[i]\n",
        "\n",
        "                        x1s, y1s, x2s, y2s = zip(*tracked_boxes)\n",
        "                        crop_x1, crop_y1 = max(0, min(x1s)), max(0, min(y1s))\n",
        "                        crop_x2, crop_y2 = min(segment[0].shape[1], max(x2s)), min(segment[0].shape[0], max(y2s))\n",
        "                        padding_x = int(0.1 * (crop_x2 - crop_x1))\n",
        "                        padding_y = int(0.05 * (crop_y2 - crop_y1))\n",
        "                        crop_x1 = max(0, crop_x1 - padding_x)\n",
        "                        crop_y1 = max(0, crop_y1 - padding_y)\n",
        "                        crop_x2 = min(segment[0].shape[1], crop_x2 + padding_x)\n",
        "                        crop_y2 = min(segment[0].shape[0], crop_y2 + padding_y)\n",
        "\n",
        "                        max_dim = max(crop_y2 - crop_y1, crop_x2 - crop_x1)\n",
        "                        if (crop_y2 - crop_y1) < max_dim:\n",
        "                            crop_y1 = (crop_y2 + crop_y1 - max_dim)//2\n",
        "                            crop_y2 = (crop_y1 + max_dim + crop_y2)//2\n",
        "                        if (crop_x2 - crop_x1) < max_dim:\n",
        "                            crop_x1 = (crop_x2 + crop_x1 - max_dim)//2\n",
        "                            crop_x2 = (crop_x1 + max_dim + crop_x2)//2\n",
        "\n",
        "                        if (crop_y2 - crop_y1) < 100 or (crop_x2 - crop_x1) < 100:\n",
        "                            print(f\"Skipping segment {k} for play_id {play_id} due to small crop size.\")\n",
        "                            continue\n",
        "\n",
        "                        final_frames = [frame[crop_y1:crop_y2, crop_x1:crop_x2] for frame in annotated_frames]\n",
        "\n",
        "                        if pitcher_name.endswith('L'):\n",
        "                            print(f\"Flipping video frames for left-handed pitcher: {pitcher_name}\")\n",
        "                            final_frames = flip_video_frames(final_frames)\n",
        "\n",
        "                        # label = process_numpy_video_for_inference(np.array(final_frames), final_class_model, play_id, num_samples=10, device='cuda')#classify video segment as pitching or not\n",
        "                        # if label != 'pitching':\n",
        "                        #     print(f\"Skipping segment {k} for play_id {play_id} as it is not a video of pitcher\")\n",
        "                        #     continue\n",
        "                        output_filename = f\"cropped_{play_id}_{k}.mp4\"\n",
        "                        output_path = os.path.join(pitcher_folder, output_filename)\n",
        "                        result_path = convert_video(np.array(final_frames), output_path)\n",
        "                        print(f\"Saved cropped video to: {result_path}\")\n",
        "                        try:\n",
        "                            if(clean_single_mp4_video(result_path)):\n",
        "                                upload_to_s3(result_path, f\"cropped_videos/{pitcher_name}/{pitch_type}/{output_filename}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error cleaning and uploading video {output_filename} for play_id {play_id}: {e}\")\n",
        "                            continue\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing segment {k} for play_id {play_id}: {e}\")\n",
        "                        continue\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in consumer for play_id {play_id}: {e}\")\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        executor.submit(producer)\n",
        "        executor.submit(consumer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#open the CSV file\n",
        "pitcher_names = pd.read_csv('top_50_pitchers_ff.csv')\n",
        "#print the unique pitcher names in main_df\n",
        "pitchers = main_df['pitcher'].unique()\n",
        "for i in pitchers:\n",
        "    if \"Carlos\" in i:\n",
        "        print(i)\n",
        "print(pitcher_names.head())\n",
        "first = True\n",
        "for i in pitcher_names['pitcher']:\n",
        "    print(i)\n",
        "    df_pitcher = main_df[main_df['pitcher'] == i]\n",
        "    if df_pitcher.empty:\n",
        "        print(f\"No data found for pitcher: {i}\")\n",
        "        continue\n",
        "    df_pitcher = df_pitcher[df_pitcher['pitch']=='FF']\n",
        "    df_pitcher = df_pitcher.sort_values(by='date', ascending=False)\n",
        "    print(f\"Processing {len(df_pitcher)} rows for pitcher: {i}\")\n",
        "    if first:\n",
        "        process_videos_from_df_numpy(df_pitcher, 0, 50, output_folder+f\"cropping_videos/\")\n",
        "        first = False\n",
        "    else:\n",
        "        process_videos_from_df_numpy(df_pitcher, 0, 50, output_folder+f\"cropping_videos/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#process_videos_from_df_numpy(df, 0, 100, output_folder+f\"cropping_videos/\")\n",
        "#find row in df where video_link contains '8b04f8f3-4dc6-4b2f-bd2e-0d9526ff4452'\n",
        "row = df[df['video_link'].str.contains('6532a138-349c-47c6-a2ff-030a7710601b')]\n",
        "print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#NOT NEEDED\n",
        "\n",
        "def process_video_from_url(video_page_url, output_folder, play_id_prefix=\"manual\"):\n",
        "    \"\"\"\n",
        "    Processes a single video URL: downloads, extracts, crops, and saves it.\n",
        "\n",
        "    Args:\n",
        "        video_page_url (str): URL to the video page.\n",
        "        output_folder (str): Directory to save the cropped videos.\n",
        "        play_id_prefix (str): Prefix for the play ID if not extracted from URL.\n",
        "    \"\"\"\n",
        "    if not video_page_url:\n",
        "        print(\"No video URL provided.\")\n",
        "        return\n",
        "\n",
        "    match = re.search(r\"playId=([a-z0-9\\-]+)\", video_page_url)\n",
        "    play_id = match.group(1) if match else play_id_prefix\n",
        "\n",
        "    print(f\"Processing video with play_id: {play_id}\")\n",
        "    video_url = get_video_url(play_id)\n",
        "    if not video_url:\n",
        "        print(f\"Could not retrieve video URL for play_id: {play_id}\")\n",
        "        return\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as tmp:\n",
        "        download_video(video_url, tmp.name)\n",
        "        temp_video_path = tmp.name\n",
        "\n",
        "    frames_array = video_to_numpy(temp_video_path)\n",
        "    print(f\"Frames array shape: {np.array(frames_array).shape}\")\n",
        "\n",
        "    gradient_array = gradient_analysis(frames_array)\n",
        "    cropped_frames = crop_video(frames_array, gradient_array)\n",
        "\n",
        "    for i, frames in enumerate(cropped_frames):\n",
        "        output_filename = os.path.join(output_folder, f\"cropped_{play_id}_{i}.mp4\")\n",
        "        output_vid = crop_video_size(np.array(frames), detect_and_draw_lowest_human)\n",
        "        convert_video(output_vid, output_filename)\n",
        "        print(f\"Cropped video saved to: {output_filename}\")\n",
        "\n",
        "    os.remove(temp_video_path)\n",
        "\n",
        "# Example usage: Process a single video URL\n",
        "video_page_url = \"https://baseballsavant.mlb.com/sporty-videos?playId=a726eb60-fc99-45b1-b57f-485d4ff95ce5\"\n",
        "output_folder = \"Video Outputs/\"\n",
        "\n",
        "process_video_from_url(video_page_url, output_folder)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
