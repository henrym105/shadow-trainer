{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Aditya Colab"
      ],
      "metadata": {
        "id": "aqcdqifhw6dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries Used\n",
        "# Core Libraries\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import tempfile\n",
        "import shutil\n",
        "import queue\n",
        "import threading\n",
        "import concurrent.futures\n",
        "\n",
        "# Data Handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Computer Vision\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Web scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "# Install required packages quietly\n",
        "!pip install -q ultralytics\n",
        "!pip install -q mediapipe\n",
        "\n",
        "# Mediapipe and YOLO\n",
        "import mediapipe as mp\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "DQXXOUMyc1yP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b5501d-61cb-432e-cfde-e539ca1b1522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCreating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VnlB4QZwt-w",
        "outputId": "c5cda1ad-154d-4642-8b27-1c6a8085c620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been loaded\n",
            "(2116274, 11)\n"
          ]
        }
      ],
      "source": [
        "# Importing and Splitting Data\n",
        "splits = {'train': 'data/train.json', 'validation': 'data/val.json', 'test': 'data/test.json'}\n",
        "df = pd.read_json(\"hf://datasets/hbfreed/Picklebot-2M/\" + splits[\"train\"])\n",
        "print(\"Data has been loaded\")\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EDA"
      ],
      "metadata": {
        "id": "8ny8SEjBUxFM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZShhW-lw5UC",
        "outputId": "67ff8dc8-e15f-4ec6-852a-83f669d5090b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique pitchers: 1881\n",
            "Count of strikes: 699313\n",
            "Count of balls: 1416961\n",
            "Number of unique pitches: 17\n",
            "Video link with specific ID: https://baseballsavant.mlb.com/sporty-videos?playId=a726eb60-fc99-45b1-b57f-485d4ff95ce5\n"
          ]
        }
      ],
      "source": [
        "#Do EDA, get number of unique pitcher, pitch types, strikes, balls\n",
        "print(\"Number of unique pitchers:\", df['pitcher'].nunique())\n",
        "print(\"Count of strikes:\", df[df['pitch_result'] != 'Ball'].shape[0])\n",
        "print(\"Count of balls:\", df[df['pitch_result'] == 'Ball'].shape[0])\n",
        "\n",
        "# Get the number of unique pitches\n",
        "print(\"Number of unique pitches:\", df['pitch'].nunique())\n",
        "\n",
        "# Get the video_link that has the string \"a726eb60-fc99-45b1-b57f-485d4ff95ce5\"\n",
        "print(\"Video link with specific ID:\", df[df['video_link'].str.contains(\"a726eb60-fc99-45b1-b57f-485d4ff95ce5\")]['video_link'].values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lNrCE0HkxH0I",
        "outputId": "a53d7359-23b9-46a6-cf22-98fd4b06dcef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        date pitch   mph spin_rate                pitcher  \\\n",
              "0 2021-05-01    FF  96.6      2478       Scott, Tanner(L)   \n",
              "1 2021-06-03    KC  80.7      2663    Workman, Brandon(R)   \n",
              "2 2023-04-25    SI  91.0      2055       Pérez, Martín(L)   \n",
              "3 2020-09-27    FF  93.5      1715  Rodriguez, Nivaldo(R)   \n",
              "4 2017-09-23    FF  99.2      2554    Ellington, Brian(R)   \n",
              "\n",
              "                 batter zone count inning pitch_result  \\\n",
              "0  Piscotty, Stephen(R)   12   1-2  Bot 8         Ball   \n",
              "1    Alvarez, Yordan(L)   13   0-2  Bot 9         Ball   \n",
              "2        Maile, Luke(R)   14   0-0  Bot 3         Ball   \n",
              "3      Odor, Rougned(L)   11   0-2  Bot 5         Ball   \n",
              "4         Lamb, Jake(L)   11   2-0  Bot 6         Ball   \n",
              "\n",
              "                                          video_link  \n",
              "0  https://baseballsavant.mlb.com/sporty-videos?p...  \n",
              "1  https://baseballsavant.mlb.com/sporty-videos?p...  \n",
              "2  https://baseballsavant.mlb.com/sporty-videos?p...  \n",
              "3  https://baseballsavant.mlb.com/sporty-videos?p...  \n",
              "4  https://baseballsavant.mlb.com/sporty-videos?p...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fe2a251f-3267-4b50-9143-730ea3f68f28\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>pitch</th>\n",
              "      <th>mph</th>\n",
              "      <th>spin_rate</th>\n",
              "      <th>pitcher</th>\n",
              "      <th>batter</th>\n",
              "      <th>zone</th>\n",
              "      <th>count</th>\n",
              "      <th>inning</th>\n",
              "      <th>pitch_result</th>\n",
              "      <th>video_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-05-01</td>\n",
              "      <td>FF</td>\n",
              "      <td>96.6</td>\n",
              "      <td>2478</td>\n",
              "      <td>Scott, Tanner(L)</td>\n",
              "      <td>Piscotty, Stephen(R)</td>\n",
              "      <td>12</td>\n",
              "      <td>1-2</td>\n",
              "      <td>Bot 8</td>\n",
              "      <td>Ball</td>\n",
              "      <td>https://baseballsavant.mlb.com/sporty-videos?p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-06-03</td>\n",
              "      <td>KC</td>\n",
              "      <td>80.7</td>\n",
              "      <td>2663</td>\n",
              "      <td>Workman, Brandon(R)</td>\n",
              "      <td>Alvarez, Yordan(L)</td>\n",
              "      <td>13</td>\n",
              "      <td>0-2</td>\n",
              "      <td>Bot 9</td>\n",
              "      <td>Ball</td>\n",
              "      <td>https://baseballsavant.mlb.com/sporty-videos?p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-04-25</td>\n",
              "      <td>SI</td>\n",
              "      <td>91.0</td>\n",
              "      <td>2055</td>\n",
              "      <td>Pérez, Martín(L)</td>\n",
              "      <td>Maile, Luke(R)</td>\n",
              "      <td>14</td>\n",
              "      <td>0-0</td>\n",
              "      <td>Bot 3</td>\n",
              "      <td>Ball</td>\n",
              "      <td>https://baseballsavant.mlb.com/sporty-videos?p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-09-27</td>\n",
              "      <td>FF</td>\n",
              "      <td>93.5</td>\n",
              "      <td>1715</td>\n",
              "      <td>Rodriguez, Nivaldo(R)</td>\n",
              "      <td>Odor, Rougned(L)</td>\n",
              "      <td>11</td>\n",
              "      <td>0-2</td>\n",
              "      <td>Bot 5</td>\n",
              "      <td>Ball</td>\n",
              "      <td>https://baseballsavant.mlb.com/sporty-videos?p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-09-23</td>\n",
              "      <td>FF</td>\n",
              "      <td>99.2</td>\n",
              "      <td>2554</td>\n",
              "      <td>Ellington, Brian(R)</td>\n",
              "      <td>Lamb, Jake(L)</td>\n",
              "      <td>11</td>\n",
              "      <td>2-0</td>\n",
              "      <td>Bot 6</td>\n",
              "      <td>Ball</td>\n",
              "      <td>https://baseballsavant.mlb.com/sporty-videos?p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe2a251f-3267-4b50-9143-730ea3f68f28')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fe2a251f-3267-4b50-9143-730ea3f68f28 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fe2a251f-3267-4b50-9143-730ea3f68f28');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-00472a41-43aa-401f-8bd8-6f26b5ee53a8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-00472a41-43aa-401f-8bd8-6f26b5ee53a8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-00472a41-43aa-401f-8bd8-6f26b5ee53a8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKVta4h-w5UD",
        "outputId": "af7b00c7-63dd-4aac-e841-5cdf813a316e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pitchers who have thrown between 1200 and 1300 pitches: ['Thompson, Zach(R)', 'Blevins, Jerry(L)', 'Chargois, JT(R)', 'Phillips, Evan(R)', 'Hutchison, Drew(R)', 'Ventura, Yordano(R)', 'Drake, Oliver(R)', 'Staumont, Josh(R)', 'Hardy, Blaine(L)', 'Crawford, Kutter(R)', 'Bowman, Matt(R)', 'Suero, Wander(R)', 'Abreu, Bryan(R)', 'Moronta, Reyes(R)', 'Wilson, Alex(R)', 'Lambert, Peter(R)', 'Brown, Hunter(R)', 'Mize, Casey(R)', 'Jurado, Ariel(R)', 'Raley, Brooks(L)', 'Bielak, Brandon(R)', 'Osuna, Roberto(R)', 'Osich, Josh(L)', 'Adleman, Tim(R)', 'Tropeano, Nick(R)', 'Kela, Keone(R)', 'Kinley, Tyler(R)', 'May, Dustin(R)', 'Sparkman, Glenn(R)', 'Erlin, Robbie(L)', 'Johnson, Brian(L)', 'Underwood Jr., Duane(R)', 'Neverauskas, Dovydas(R)', 'Otero, Dan(R)', 'Middleton, Keynan(R)', 'Adams, Austin(R)', 'Bickford, Phil(R)', 'Falter, Bailey(L)', 'Payamps, Joel(R)', 'Oswalt, Corey(R)', 'Domínguez, Seranthony(R)']\n"
          ]
        }
      ],
      "source": [
        "#get pitcher names who have thrown between 1200 and 1300 pitches\n",
        "pitcher_counts = df['pitcher'].value_counts()\n",
        "pitchers_1200_1300 = pitcher_counts[(pitcher_counts >= 1200) & (pitcher_counts <= 1300)].index.tolist()\n",
        "print(\"Pitchers who have thrown between 1200 and 1300 pitches:\", pitchers_1200_1300)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of pitchers between 1200 & 1300\n",
        "len(pitchers_1200_1300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzs-TbvpfF7g",
        "outputId": "cbe5dbfb-51f9-4778-cbe2-bbf2144be141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KdGc3pOxKQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28c7e3fb-7869-4965-9560-b330bee0d663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique pitch types: ['FF' 'KC' 'SI' 'SL' 'CU' 'CH' 'FC' 'FA' 'ST' 'FS' '' 'SV' 'KN' 'EP' 'SC' 'FO' 'CS']\n"
          ]
        }
      ],
      "source": [
        "#get all unique pitch types\n",
        "pitch_types = df['pitch'].unique()\n",
        "print(\"Unique pitch types:\", pitch_types)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filter for specific pitcher\n",
        "df = df[df['pitcher'] == 'May, Dustin(R)']"
      ],
      "metadata": {
        "id": "i11GfPjBT_Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPoywu2lxT_F",
        "outputId": "0e29a1d3-69ed-4da2-861d-355c412f1391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1233, 11)\n",
            "object\n",
            "Shape after dropping NA: (1233, 11)\n",
            "Average Speed (mph): 94.00145985401458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-3419719854>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['mph'] = pd.to_numeric(df['mph'], errors='coerce')\n"
          ]
        }
      ],
      "source": [
        "#get size\n",
        "print(df.shape)\n",
        "\n",
        "#get the data type of mph\n",
        "print(df['mph'].dtype)\n",
        "\n",
        "#convert mph to a float from a string\n",
        "df['mph'] = pd.to_numeric(df['mph'], errors='coerce')\n",
        "df = df.dropna(subset=['mph'])\n",
        "\n",
        "print(\"Shape after dropping NA:\", df.shape)\n",
        "print(\"Average Speed (mph):\", df['mph'].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "r_g7h1l6xpkD",
        "outputId": "cff9c031-d475-41d1-87c2-473f6b820c69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pitch_result\n",
              "Ball             796\n",
              "Called Strike    437\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pitch_result</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Ball</th>\n",
              "      <td>796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Called Strike</th>\n",
              "      <td>437</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#get counts of pitch_result\n",
        "df['pitch_result'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llPmPcFZ0PLr",
        "outputId": "fdc8ea63-0979-45d1-aec7-413f757896f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    1233.000000\n",
            "mean       94.001460\n",
            "std         4.565858\n",
            "min        81.100000\n",
            "25%        91.300000\n",
            "50%        95.700000\n",
            "75%        97.600000\n",
            "max       100.500000\n",
            "Name: mph, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "#print the min, 25th, 50th, 75th and max percentiles of mph\n",
        "print(df['mph'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St67eu_DXEkH"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This script provides utility functions for downloading and processing baseball play videos\n",
        "from MLB's Baseball Savant website.\n",
        "\n",
        "Given a play ID or URL, it can:\n",
        "1. Extract the direct video URL from the Baseball Savant webpage.\n",
        "2. Download the video to a local file.\n",
        "3. Convert the video into a NumPy array of frames, optionally resized or frame-limited.\n",
        "4. Save the resulting NumPy array to disk for downstream machine learning or computer vision tasks.\n",
        "\"\"\"\n",
        "\n",
        "def get_video_url(play_id):\n",
        "    # Given a play_id, constructs the MLB Baseball Savant video URL\n",
        "    url = f\"https://baseballsavant.mlb.com/sporty-videos?playId={play_id}\"\n",
        "\n",
        "    # Makes an HTTP GET request to fetch the HTML content of the page\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Looks for the <video> tag in the HTML\n",
        "    video_tag = soup.find(\"video\")\n",
        "    if video_tag:\n",
        "        # Within the video tag, find the <source> tag\n",
        "        source_tag = video_tag.find(\"source\")\n",
        "        # If a source with a valid `src` attribute is found, return the video URL\n",
        "        if source_tag and source_tag.get(\"src\"):\n",
        "            return source_tag[\"src\"]\n",
        "    return None  # Return None if no video URL is found\n",
        "\n",
        "\n",
        "def get_video(url):\n",
        "    # Same as get_video_url, but starts from a full URL instead of play_id\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    video_tag = soup.find(\"video\")\n",
        "    if video_tag:\n",
        "        source_tag = video_tag.find(\"source\")\n",
        "        if source_tag and source_tag.get(\"src\"):\n",
        "            return source_tag[\"src\"]\n",
        "    return None\n",
        "\n",
        "\n",
        "def download_video(video_url, output_file=\"temp.mp4\"):\n",
        "    # Downloads the video file from the given URL and saves it locally\n",
        "    r = requests.get(video_url, stream=True)\n",
        "    with open(output_file, \"wb\") as f:\n",
        "        # Streams and writes the content in 1MB chunks\n",
        "        for chunk in r.iter_content(chunk_size=1024*1024):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "\n",
        "def video_to_numpy(video_path, resize=None, max_frames=None):\n",
        "    # Converts a video file to a numpy array of frames\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break  # Stop if no frame is read (end of video)\n",
        "        if resize:\n",
        "            # Resize the frame if a target size is provided\n",
        "            frame = cv2.resize(frame, resize)\n",
        "        frames.append(frame)\n",
        "        count += 1\n",
        "        if max_frames and count >= max_frames:\n",
        "            break  # Stop after reading `max_frames` frames\n",
        "\n",
        "    cap.release()  # Release video capture object\n",
        "    return np.array(frames)  # Return frames as a numpy array\n",
        "\n",
        "\n",
        "def save_numpy_array(array, filename=\"video_frames.npy\"):\n",
        "    # Saves a numpy array to a .npy file on disk\n",
        "    np.save(filename, array)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usVqfTqK-jXH"
      },
      "source": [
        "Prompt:\n",
        "The seconds when the pitcher is possibly in the middle, and the location of the pitchers change depending on the video. We may have to use ML for this.\n",
        "\n",
        "\n",
        "\n",
        "ChatGPT said:\n",
        "You're absolutely right — since the pitcher’s timing and location vary from video to video, a fixed crop won't work well. This is a classic use case for computer vision with machine learning. Here’s a structured way to approach it:\n",
        "\n",
        "✅ Goal:\n",
        "Automatically detect and crop the pitcher across different videos with varying camera angles and timings.\n",
        "\n",
        "🧠 Solution Options:\n",
        "Option 1: Pose Estimation (Recommended)\n",
        "Use a pre-trained pose estimation model like:\n",
        "\n",
        "OpenPose\n",
        "\n",
        "MediaPipe Pose (Google)\n",
        "\n",
        "YOLO + Keypoint detection\n",
        "\n",
        "✅ Pros:\n",
        "\n",
        "Accurate for identifying human bodies.\n",
        "\n",
        "Can isolate the pitcher based on pose and movement patterns.\n",
        "\n",
        "Option 2: Object Detection\n",
        "Train or use a model (like YOLOv5, YOLOv8, or Detectron2) to detect “pitcher” as a custom class.\n",
        "\n",
        "✅ Pros:\n",
        "\n",
        "Works on frame-by-frame basis.\n",
        "\n",
        "More precise than pose estimation in cluttered scenes.\n",
        "\n",
        "❌ Cons:\n",
        "\n",
        "Requires labeled data (you may need to annotate a small dataset of frames showing pitchers).\n",
        "\n",
        "🔧 Workflow Example (with MediaPipe Pose in Colab):\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose()\n",
        "\n",
        "def detect_pitcher_bbox(video_path, max_frames=100):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    bboxes = []\n",
        "\n",
        "    for i in range(max_frames):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        h, w, _ = frame.shape\n",
        "\n",
        "        # Run pose estimation\n",
        "        results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        if results.pose_landmarks:\n",
        "            # Get min/max of landmark coordinates for bounding box\n",
        "            x_coords = [lmk.x for lmk in results.pose_landmarks.landmark]\n",
        "            y_coords = [lmk.y for lmk in results.pose_landmarks.landmark]\n",
        "            xmin, xmax = min(x_coords), max(x_coords)\n",
        "            ymin, ymax = min(y_coords), max(y_coords)\n",
        "\n",
        "            # Convert to pixel coordinates\n",
        "            x, y = int(xmin * w), int(ymin * h)\n",
        "            width, height = int((xmax - xmin) * w), int((ymax - ymin) * h)\n",
        "            bboxes.append((x, y, width, height))\n",
        "\n",
        "    cap.release()\n",
        "    return bboxes\n",
        "This gives you approximate bounding boxes of the pitcher from the first max_frames frames.\n",
        "\n",
        "🧪 Next Steps:\n",
        "You can either:\n",
        "\n",
        "Crop each frame using the detected bounding box.\n",
        "\n",
        "Track the pitcher across frames.\n",
        "\n",
        "Save the cropped regions into a new video or a NumPy array.\n",
        "\n",
        "Let me know if you want help setting up pose-based cropping, YOLO training with pitcher labels, or a pipeline to auto-crop and save video segments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s-5hli1tzeT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "748d0289-5056-42c2-b994-1e70bcdf1fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.35M/5.35M [00:00<00:00, 105MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Initialize model once globally\n",
        "model = YOLO('yolo11n.pt')  # You can replace this with another YOLO model variant\n",
        "\n",
        "def detect_and_draw_all_humans(frame):\n",
        "    \"\"\"\n",
        "    Detects all humans in the frame using YOLOv8.\n",
        "    Crops 20% off each horizontal side before detection for focus.\n",
        "\n",
        "    Args:\n",
        "        frame (np.ndarray): BGR image frame from a video.\n",
        "\n",
        "    Returns:\n",
        "        frame (np.ndarray): The original frame (unaltered visually).\n",
        "        detected_boxes (List[Tuple[int, int, int, int, str]]): Bounding boxes of detected people\n",
        "            in (x1, y1, x2, y2, 'person') format in original frame coordinates.\n",
        "    \"\"\"\n",
        "    h, w, _ = frame.shape\n",
        "    left_crop = int(w * 0.2)\n",
        "    right_crop = int(w * 0.8)\n",
        "\n",
        "    # Crop 20% from both left and right sides for focused detection\n",
        "    cropped_frame = frame[:, left_crop:right_crop]\n",
        "\n",
        "    # Convert to RGB as required by YOLOv8\n",
        "    rgb_frame = cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Run YOLOv8 detection using GPU if available\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    results = model.predict(rgb_frame, device=device, verbose=False)[0]\n",
        "\n",
        "    detected_boxes = []\n",
        "    for box in results.boxes:\n",
        "        cls_id = int(box.cls[0])\n",
        "        cls_name = model.names[cls_id]\n",
        "        if cls_name == 'person':\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            # Adjust x-coordinates back to original frame\n",
        "            x1 += left_crop\n",
        "            x2 += left_crop\n",
        "\n",
        "            # Optional: draw rectangle if you want to visualize\n",
        "            # cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            detected_boxes.append((x1, y1, x2, y2, cls_name))\n",
        "\n",
        "    return frame, detected_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXBQouBfY75A"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This script processes videos from a folder by:\n",
        "1. Converting them into numpy arrays.\n",
        "2. Performing frame-wise similarity analysis using SSIM to detect transitions.\n",
        "3. Cropping segments with meaningful action based on similarity thresholds.\n",
        "4. Detecting and tracking the main person (pitcher) using YOLOv11.\n",
        "5. Cropping and saving the final annotated video segments.\n",
        "\"\"\"\n",
        "\n",
        "# Load YOLOv11 model globally (auto selects GPU if available)\n",
        "model = YOLO('yolo11n.pt')\n",
        "\n",
        "# ----------- UTILITY FUNCTIONS ----------- #\n",
        "def ssim_diff(frame1, frame2):\n",
        "    grayA = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
        "    grayB = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
        "    score, _ = ssim(grayA, grayB, full=True)\n",
        "    return score\n",
        "\n",
        "def gradient_analysis(frames_array):\n",
        "    \"\"\"Computes SSIM-based similarity gradients across video frames.\"\"\"\n",
        "    grad_arr = []\n",
        "    for i in range(1, len(frames_array)):\n",
        "        grad = ssim_diff(cv2.resize(frames_array[i], (100,100)), cv2.resize(frames_array[i-1], (100,100)))\n",
        "        grad_arr.append(grad)\n",
        "    return grad_arr\n",
        "\n",
        "def crop_video(frames_array, grad_arr):\n",
        "    \"\"\"Recursively crops frames based on sharp changes in similarity.\"\"\"\n",
        "    if frames_array is None or len(frames_array) <= 10:\n",
        "        return [frames_array]\n",
        "    for i in range(len(grad_arr)):\n",
        "        if grad_arr[i] < 0.4:\n",
        "            return [frames_array[:i]] + crop_video(frames_array[i+2:], grad_arr[i+2:])\n",
        "    return [frames_array]\n",
        "\n",
        "def convert_video(frames_array, output_path):\n",
        "    \"\"\"Saves frames_array as a video to the given output_path.\"\"\"\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, 20.0, (frames_array.shape[2], frames_array.shape[1]))\n",
        "    for frame in frames_array:\n",
        "        out.write(frame)\n",
        "    out.release()\n",
        "    return output_path\n",
        "\n",
        "def video_to_numpy(video_path, resize=None, max_frames=None):\n",
        "    \"\"\"Converts a video file into a numpy array of frames.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    count = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if resize:\n",
        "            frame = cv2.resize(frame, resize)\n",
        "        frames.append(frame)\n",
        "        count += 1\n",
        "        if max_frames and count >= max_frames:\n",
        "            break\n",
        "    cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "def detect_and_draw_all_humans(frame):\n",
        "    \"\"\"Detects all humans in a cropped portion of the frame using YOLOv8.\"\"\"\n",
        "    h, w, _ = frame.shape\n",
        "    left_crop = int(w * 0.2)\n",
        "    right_crop = int(w * 0.8)\n",
        "    cropped_frame = frame[:, left_crop:right_crop]\n",
        "    rgb_frame = cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2RGB)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    results = model.predict(rgb_frame, device=device, verbose=False)[0]\n",
        "\n",
        "    detected_boxes = []\n",
        "    for box in results.boxes:\n",
        "        cls_id = int(box.cls[0])\n",
        "        cls_name = model.names[cls_id]\n",
        "        if cls_name == 'person':\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            x1 += left_crop\n",
        "            x2 += left_crop\n",
        "            detected_boxes.append((x1, y1, x2, y2, cls_name))\n",
        "    return frame, detected_boxes\n",
        "\n",
        "# ----------- MAIN PROCESSING FUNCTION ----------- #\n",
        "def process_videos_in_folder(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Processes all .mp4 videos from input_folder, detects main person,\n",
        "    crops and saves segments with significant motion.\n",
        "    \"\"\"\n",
        "    if not os.path.isdir(input_folder):\n",
        "        print(f\"Error: Input folder not found at {input_folder}\")\n",
        "        return\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    video_files = [f for f in os.listdir(input_folder) if f.endswith('.mp4')]\n",
        "    if not video_files:\n",
        "        print(f\"No MP4 videos found in {input_folder}\")\n",
        "        return\n",
        "\n",
        "    for video_file in video_files:\n",
        "        video_path = os.path.join(input_folder, video_file)\n",
        "        print(f\"Processing video: {video_path}\")\n",
        "\n",
        "        frames_array = video_to_numpy(video_path)\n",
        "        gradient_array = gradient_analysis(frames_array)\n",
        "        cropped_frames = crop_video(frames_array, gradient_array)\n",
        "\n",
        "        for i, frames_segment in enumerate(cropped_frames):\n",
        "            if not frames_segment:\n",
        "                continue\n",
        "\n",
        "            mid_index = len(frames_segment) // 2\n",
        "            mid_frame = frames_segment[mid_index]\n",
        "            _, mid_detected_boxes = detect_and_draw_all_humans(mid_frame)\n",
        "\n",
        "            candidates = [\n",
        "                (int(x1), int(y1), int(x2), int(y2))\n",
        "                for (x1, y1, x2, y2, cls) in mid_detected_boxes if cls == 'person']\n",
        "\n",
        "            if not candidates:\n",
        "                print(\"No humans in middle frame. Skipping segment.\")\n",
        "                continue\n",
        "\n",
        "            def height(b): return b[3] - b[1]\n",
        "            selected_box = max(candidates, key=height)\n",
        "\n",
        "            def iou(b1, b2):\n",
        "                xa, ya = max(b1[0], b2[0]), max(b1[1], b2[1])\n",
        "                xb, yb = min(b1[2], b2[2]), min(b1[3], b2[3])\n",
        "                inter_area = max(0, xb - xa) * max(0, yb - ya)\n",
        "                area1 = (b1[2] - b1[0]) * (b1[3] - b1[1])\n",
        "                area2 = (b2[2] - b2[0]) * (b2[3] - b2[1])\n",
        "                union_area = area1 + area2 - inter_area\n",
        "                return inter_area / union_area if union_area > 0 else 0\n",
        "\n",
        "            bounding_boxes, annotated_frames = [], []\n",
        "            for frame in frames_segment:\n",
        "                annotated_frame, detected_boxes = detect_and_draw_all_humans(frame)\n",
        "                candidates = [\n",
        "                    (int(x1), int(y1), int(x2), int(y2))\n",
        "                    for (x1, y1, x2, y2, cls) in detected_boxes if cls == 'person']\n",
        "\n",
        "                best_box = max(candidates, key=lambda b: iou(b, selected_box), default=None) if candidates else None\n",
        "                if best_box and iou(best_box, selected_box) >= 0.5:\n",
        "                    bounding_boxes.append(best_box)\n",
        "                else:\n",
        "                    bounding_boxes.append(selected_box)\n",
        "                annotated_frames.append(annotated_frame)\n",
        "\n",
        "            if not bounding_boxes:\n",
        "                print(\"No valid bounding boxes. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            x1s, y1s, x2s, y2s = zip(*bounding_boxes)\n",
        "            crop_x1, crop_y1 = max(0, min(x1s)), max(0, min(y1s))\n",
        "            crop_x2, crop_y2 = min(frame.shape[1], max(x2s)), min(frame.shape[0], max(y2s))\n",
        "\n",
        "            if crop_y2 - crop_y1 > 550:\n",
        "                print(\"Cropped region too tall. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            final_frames = [frame[crop_y1:crop_y2, crop_x1:crop_x2] for frame in annotated_frames]\n",
        "            output_filename = f\"cropped_{i}_{video_file}\"\n",
        "            output_path = os.path.join(output_folder, output_filename)\n",
        "            convert_video(np.array(final_frames), output_path)\n",
        "            print(f\"Saved cropped video: {output_path}\")\n",
        "\n",
        "            # Free memory\n",
        "            del frames_segment, final_frames, bounding_boxes, annotated_frames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8Eh6bKgw5UF"
      },
      "outputs": [],
      "source": [
        "def average_sampled_frames_from_array(frame_array, num_samples=10):\n",
        "    \"\"\"\n",
        "    Averages sampled frames from a NumPy array of frames.\n",
        "\n",
        "    Args:\n",
        "        frame_array (np.ndarray): A NumPy array of shape (num_frames, height, width, channels).\n",
        "        num_samples (int): Number of frames to sample and average.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The averaged frame resized to (100, 100).\n",
        "    \"\"\"\n",
        "    total_frames = frame_array.shape[0]\n",
        "    if total_frames < num_samples:\n",
        "        raise ValueError(\"Array has fewer frames than the number of samples requested.\")\n",
        "\n",
        "    indices = np.linspace(0, total_frames - 1, num_samples, dtype=int)\n",
        "    sampled_frames = frame_array[indices].astype(np.float32)\n",
        "\n",
        "    avg_frame = np.mean(sampled_frames, axis=0)\n",
        "    avg_frame = avg_frame.astype(np.uint8)\n",
        "\n",
        "    # Resize to 100x100\n",
        "    avg_frame = cv2.resize(avg_frame, (100, 100), interpolation=cv2.INTER_AREA)\n",
        "    return avg_frame\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "v89Vgayb1Usz",
        "outputId": "66bb242a-d62a-442f-ca41-6e0ab596a1a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-560cb3c1-46a2-474b-b388-de1e5a570aab\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-560cb3c1-46a2-474b-b388-de1e5a570aab\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cnn_model.pth to cnn_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7G_Q6zDw5UG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caadf49b-b6e3-43d1-9fd0-ad096a230417"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleCNN(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=20000, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a simple CNN model architecture for binary classification\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # First convolutional layer: 3 input channels (RGB), 16 output channels, 3x3 kernel\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Downsamples by factor of 2\n",
        "        # Second convolutional layer: 16 input channels, 32 output channels\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        # Fully connected layer: flattening output of conv2 layer (assumes input size 100x100 -> 25x25 after pooling)\n",
        "        self.fc1 = nn.Linear(32 * 25 * 25, 64)  # Intermediate dense layer\n",
        "        self.fc2 = nn.Linear(64, 2)             # Final layer for 2 output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # -> (B, 16, 50, 50)\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # -> (B, 32, 25, 25)\n",
        "        x = x.view(-1, 32 * 25 * 25)           # Flatten before fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)                        # Output logits (for use with CrossEntropyLoss)\n",
        "        return x\n",
        "\n",
        "# ------- MODEL LOADING FOR INFERENCE ------- #\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "class_model = SimpleCNN()\n",
        "\n",
        "# Load the trained weights\n",
        "class_model.load_state_dict(torch.load(\"cnn_model.pth\", map_location=device))\n",
        "\n",
        "# Move model to correct device\n",
        "class_model = class_model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode to deactivate dropout/batchnorm\n",
        "class_model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Device:\", torch.cuda.get_device_name(0))\n",
        "    print(\"CUDA Version:\", torch.version.cuda)\n",
        "    print(\"Device Count:\", torch.cuda.device_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2ZCJrn3f6aK",
        "outputId": "5cb94033-893f-4d07-a1fc-642553c997f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available: True\n",
            "GPU Device: Tesla T4\n",
            "CUDA Version: 12.4\n",
            "Device Count: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOlsZX5yw5UG"
      },
      "outputs": [],
      "source": [
        "def process_numpy_video_for_inference(frame_array, class_model, video_name, num_samples=10, device='cuda'):\n",
        "    \"\"\"\n",
        "    Processes a NumPy array of video frames for inference using the given model.\n",
        "\n",
        "    Args:\n",
        "        frame_array (np.ndarray): NumPy array of shape (num_frames, height, width, channels), in RGB format.\n",
        "        model (torch.nn.Module): Trained PyTorch model for inference.\n",
        "        video_name (str): The name of the original video (used for saving output).\n",
        "        output_folder (str): Path to the output folder where results will be saved.\n",
        "        num_samples (int): Number of frames to sample and average.\n",
        "        device (str): 'cuda' or 'cpu'\n",
        "\n",
        "    Returns:\n",
        "        str: Predicted label ('pitching' or 'other').\n",
        "    \"\"\"\n",
        "    try:\n",
        "\n",
        "        # Compute average frame\n",
        "        avg_frame = average_sampled_frames_from_array(frame_array, num_samples)\n",
        "        avg_frame_tensor = torch.tensor(avg_frame).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
        "        avg_frame_tensor = avg_frame_tensor.to(device)\n",
        "\n",
        "        # Predict\n",
        "        class_model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = class_model(avg_frame_tensor)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "\n",
        "        label = 'pitching' if predicted.item() == 1 else 'other'\n",
        "\n",
        "\n",
        "        print(f\"Processed {video_name}: {label}\")\n",
        "        return label\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {video_name}: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5TG_28lh42R"
      },
      "outputs": [],
      "source": [
        "# Update output folder path\n",
        "output_folder = \"Video Outputs/\"\n",
        "\n",
        "\n",
        "def crop_video_size(frames_array, detect_and_draw_fn):\n",
        "    \"\"\"\n",
        "    Processes a video in numpy array format, detects the lowest human in each frame,\n",
        "    crops all frames to the bounding box covering all detected humans, and returns\n",
        "    the cropped video as a numpy array.\n",
        "\n",
        "    Args:\n",
        "        frames_array (np.ndarray): Input video as a numpy array (frames, H, W, C).\n",
        "        detect_and_draw_fn (function): Function to detect and return (frame, bbox) for each frame.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Cropped video as a numpy array.\n",
        "    \"\"\"\n",
        "    bounding_boxes = []\n",
        "    annotated_frames = []\n",
        "\n",
        "    for frame in frames_array:\n",
        "        annotated_frame, bbox = detect_and_draw_fn(frame)\n",
        "        if bbox:\n",
        "            bounding_boxes.append(bbox)\n",
        "            annotated_frames.append(annotated_frame)\n",
        "\n",
        "    if not bounding_boxes:\n",
        "        print(\"No humans detected in this video. Returning original frames.\")\n",
        "        return frames_array\n",
        "\n",
        "    x1s, y1s, x2s, y2s = zip(*bounding_boxes)\n",
        "    crop_x1, crop_y1 = max(0, min(x1s)), max(0, min(y1s))\n",
        "    crop_x2, crop_y2 = min(frames_array.shape[2], max(x2s)), min(frames_array.shape[1], max(y2s))\n",
        "\n",
        "    final_frames = [\n",
        "        frame[crop_y1:crop_y2, crop_x1:crop_x2] for frame in annotated_frames\n",
        "    ]\n",
        "\n",
        "    return np.array(final_frames)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to flip the video frames horizontally\n",
        "def flip_video_frames(frames_array):\n",
        "    \"\"\"\n",
        "    Flips the video frames horizontally.\n",
        "\n",
        "    Args:\n",
        "        frames_array (np.ndarray): Input video as a numpy array (frames, H, W, C).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Flipped video as a numpy array.\n",
        "    \"\"\"\n",
        "    return np.flip(frames_array, axis=2)  # Flip along the width dimension"
      ],
      "metadata": {
        "id": "jiofusTucLl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "print(f\"Reserved memory:  {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35NNTa7EhtUf",
        "outputId": "ab4a271e-b463-457f-e4a7-984d9bc34849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allocated memory: 4.90 MB\n",
            "Reserved memory:  22.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Processing Function"
      ],
      "metadata": {
        "id": "CfV59kccgB3n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD4S1TAGw5UH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0b1c53a-ab33-4f56-a224-04aa91a6d84b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing 8b3a1b77-4907-4928-9ac5-baec37877046: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id 8b3a1b77-4907-4928-9ac5-baec37877046 as it is not a pitching video.\n"
          ]
        }
      ],
      "source": [
        "def process_videos_from_df_numpy(df, starting_row, n_rows, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    task_queue = queue.Queue(maxsize=1)\n",
        "    stop_signal = object()\n",
        "\n",
        "    def producer():\n",
        "        print(f\"Starting producer thread for rows {starting_row} to {starting_row + n_rows}\")\n",
        "        for index, row in df.iterrows():\n",
        "            print(f\"Processing row index: {index}, pitcher: {row.get('pitcher')}, video_link: {row.get('video_link')}\")\n",
        "            if index < starting_row:\n",
        "                continue\n",
        "\n",
        "            pitcher_name = row.get('pitcher')\n",
        "            pitcher_name = re.sub(r'\\W+', '', pitcher_name) if pitcher_name else \"UnknownPitcher\"\n",
        "            pitcher_folder = os.path.join(output_folder, pitcher_name)\n",
        "            os.makedirs(pitcher_folder, exist_ok=True)\n",
        "\n",
        "            video_page_url = row.get('video_link')\n",
        "            if pd.isna(video_page_url) or not video_page_url:\n",
        "                print(f\"Skipping row {index} due to missing video_link.\")\n",
        "                continue\n",
        "\n",
        "            match = re.search(r\"playId=([a-z0-9\\-]+)\", video_page_url)\n",
        "            if not match:\n",
        "                print(f\"Could not extract playId from URL: {video_page_url}\")\n",
        "                continue\n",
        "\n",
        "            play_id = match.group(1)\n",
        "            video_url = get_video_url(play_id)\n",
        "\n",
        "            if not video_url:\n",
        "                print(f\"No video URL found for playId: {play_id}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as tmp:\n",
        "                    download_video(video_url, tmp.name)\n",
        "                    temp_video_path = tmp.name\n",
        "\n",
        "                frames_array = video_to_numpy(temp_video_path)\n",
        "                os.remove(temp_video_path)\n",
        "                task_queue.put((index, row, frames_array, play_id, pitcher_folder, pitcher_name))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in producer for play_id {play_id}: {e}\")\n",
        "                continue\n",
        "\n",
        "        task_queue.put(stop_signal)\n",
        "\n",
        "    def consumer():\n",
        "        while True:\n",
        "            item = task_queue.get()\n",
        "            if item is stop_signal:\n",
        "                break\n",
        "\n",
        "            index, row, frames_array, play_id, pitcher_folder, pitcher_name = item\n",
        "            try:\n",
        "                gradient_array = gradient_analysis(frames_array)\n",
        "                cropped_segments = crop_video(frames_array, gradient_array)\n",
        "\n",
        "                for k, segment in enumerate(cropped_segments):\n",
        "                    try:\n",
        "                        label = process_numpy_video_for_inference(segment, class_model, play_id, num_samples=10, device=next(class_model.parameters()).device)\n",
        "                        if label != 'pitching':\n",
        "                            print(f\"Skipping segment {k} for play_id {play_id} as it is not a pitching video.\")\n",
        "                            continue\n",
        "\n",
        "                        mid_index = len(segment) // 2\n",
        "                        mid_frame = segment[mid_index]\n",
        "                        mid_annotated, mid_boxes = detect_and_draw_all_humans(mid_frame)\n",
        "                        candidates = [(int(x1), int(y1), int(x2), int(y2)) for (x1, y1, x2, y2, cls) in mid_boxes if cls == 'person']\n",
        "                        candidates = [b for b in candidates if (b[3] - b[1]) > 75 and (b[2] - b[0]) > 75]\n",
        "\n",
        "                        if not candidates:\n",
        "                            print(\"No humans found in the middle frame. Skipping video.\")\n",
        "                            continue\n",
        "\n",
        "                        def midpoint(b):\n",
        "                          return (b[0] + b[2]) / 2, (b[1] + b[3]) / 2\n",
        "                        mid_y = int(mid_frame.shape[0] * 0.65)\n",
        "                        selected_box = min(candidates, key=lambda b: abs(midpoint(b)[1] - mid_y))\n",
        "                        selected_box = tuple(map(int, selected_box))\n",
        "                        print(f\"Selected box for tracking: {selected_box}\")\n",
        "\n",
        "                        def iou(b1, b2):\n",
        "                            xa, ya = max(b1[0], b2[0]), max(b1[1], b2[1])\n",
        "                            xb, yb = min(b1[2], b2[2]), min(b1[3], b2[3])\n",
        "                            inter_area = max(0, xb - xa) * max(0, yb - ya)\n",
        "                            area1 = (b1[2] - b1[0]) * (b1[3] - b1[1])\n",
        "                            area2 = (b2[2] - b2[0]) * (b2[3] - b2[1])\n",
        "                            union_area = area1 + area2 - inter_area\n",
        "                            return inter_area / union_area if union_area > 0 else 0\n",
        "\n",
        "                        tracked_boxes = [None] * len(segment)\n",
        "                        tracked_boxes[mid_index] = selected_box\n",
        "                        annotated_frames = [None] * len(segment)\n",
        "\n",
        "                        prev_box = selected_box\n",
        "                        for i in range(mid_index, len(segment)):\n",
        "                            frame = segment[i]\n",
        "                            annotated, detected_boxes = detect_and_draw_all_humans(frame)\n",
        "                            annotated_frames[i] = annotated\n",
        "                            candidates = [(int(x1), int(y1), int(x2), int(y2)) for (x1, y1, x2, y2, cls) in detected_boxes if cls == 'person']\n",
        "                            best_box = max(candidates, key=lambda b: iou(b, prev_box), default=None) if candidates else None\n",
        "                            tracked_boxes[i] = best_box if best_box and iou(best_box, prev_box) >= 0.75 else prev_box\n",
        "                            prev_box = tracked_boxes[i]\n",
        "\n",
        "                        prev_box = selected_box\n",
        "                        for i in range(mid_index - 1, -1, -1):\n",
        "                            frame = segment[i]\n",
        "                            annotated, detected_boxes = detect_and_draw_all_humans(frame)\n",
        "                            annotated_frames[i] = annotated\n",
        "                            candidates = [(int(x1), int(y1), int(x2), int(y2)) for (x1, y1, x2, y2, cls) in detected_boxes if cls == 'person']\n",
        "                            best_box = max(candidates, key=lambda b: iou(b, prev_box), default=None) if candidates else None\n",
        "                            tracked_boxes[i] = best_box if best_box and iou(best_box, prev_box) >= 0.75 else prev_box\n",
        "                            prev_box = tracked_boxes[i]\n",
        "\n",
        "                        x1s, y1s, x2s, y2s = zip(*tracked_boxes)\n",
        "                        crop_x1, crop_y1 = max(0, min(x1s)), max(0, min(y1s))\n",
        "                        crop_x2, crop_y2 = min(segment[0].shape[1], max(x2s)), min(segment[0].shape[0], max(y2s))\n",
        "                        padding_x = int(0.1 * (crop_x2 - crop_x1))\n",
        "                        padding_y = int(0.05 * (crop_y2 - crop_y1))\n",
        "                        crop_x1 = max(0, crop_x1 - padding_x)\n",
        "                        crop_y1 = max(0, crop_y1 - padding_y)\n",
        "                        crop_x2 = min(segment[0].shape[1], crop_x2 + padding_x)\n",
        "                        crop_y2 = min(segment[0].shape[0], crop_y2 + padding_y)\n",
        "\n",
        "                        max_dim = max(crop_y2 - crop_y1, crop_x2 - crop_x1)\n",
        "                        if (crop_y2 - crop_y1) < max_dim:\n",
        "                            crop_y1 = (crop_y2 + crop_y1 - max_dim)//2\n",
        "                            crop_y2 = (crop_y1 + max_dim + crop_y2)//2\n",
        "                        if (crop_x2 - crop_x1) < max_dim:\n",
        "                            crop_x1 = (crop_x2 + crop_x1 - max_dim)//2\n",
        "                            crop_x2 = (crop_x1 + max_dim + crop_x2)//2\n",
        "\n",
        "                        if (crop_y2 - crop_y1) < 100 or (crop_x2 - crop_x1) < 100:\n",
        "                            print(f\"Skipping segment {k} for play_id {play_id} due to small crop size.\")\n",
        "                            continue\n",
        "\n",
        "                        final_frames = [frame[crop_y1:crop_y2, crop_x1:crop_x2] for frame in annotated_frames]\n",
        "\n",
        "                        if pitcher_name.endswith('L'):\n",
        "                            print(f\"Flipping video frames for left-handed pitcher: {pitcher_name}\")\n",
        "                            final_frames = flip_video_frames(final_frames)\n",
        "\n",
        "                        output_filename = f\"cropped_{play_id}_{k}.mp4\"\n",
        "                        output_path = os.path.join(pitcher_folder, output_filename)\n",
        "                        result_path = convert_video(np.array(final_frames), output_path)\n",
        "                        print(f\"Saved cropped video to: {result_path}\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing segment {k} for play_id {play_id}: {e}\")\n",
        "                        continue\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in consumer for play_id {play_id}: {e}\")\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        executor.submit(producer)\n",
        "        executor.submit(consumer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8XKFUiGw5UH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "38deda53-60cf-4e6b-bc3a-f3a76f8a9b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting producer thread for rows 0 to 10\n",
            "Processing row index: 121, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=f5d533d9-0dec-44cf-9e44-1763025eeeb2\n",
            "Processing row index: 903, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=e8ffa775-fbb7-49be-99cd-b92484b1d18b\n",
            "Error processing f5d533d9-0dec-44cf-9e44-1763025eeeb2: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id f5d533d9-0dec-44cf-9e44-1763025eeeb2 as it is not a pitching video.\n",
            "Error processing f5d533d9-0dec-44cf-9e44-1763025eeeb2: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 1 for play_id f5d533d9-0dec-44cf-9e44-1763025eeeb2 as it is not a pitching video.\n",
            "Processing row index: 1188, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=e3058963-3dc0-4eaa-92d2-0ccb043ac574\n",
            "Error processing e8ffa775-fbb7-49be-99cd-b92484b1d18b: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id e8ffa775-fbb7-49be-99cd-b92484b1d18b as it is not a pitching video.\n",
            "Error processing e8ffa775-fbb7-49be-99cd-b92484b1d18b: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 1 for play_id e8ffa775-fbb7-49be-99cd-b92484b1d18b as it is not a pitching video.\n",
            "Error processing e8ffa775-fbb7-49be-99cd-b92484b1d18b: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 2 for play_id e8ffa775-fbb7-49be-99cd-b92484b1d18b as it is not a pitching video.\n",
            "Processing row index: 7240, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=7cc90da8-7e63-4074-bcbc-bc327be0d749\n",
            "Error processing e3058963-3dc0-4eaa-92d2-0ccb043ac574: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id e3058963-3dc0-4eaa-92d2-0ccb043ac574 as it is not a pitching video.\n",
            "Error processing e3058963-3dc0-4eaa-92d2-0ccb043ac574: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 1 for play_id e3058963-3dc0-4eaa-92d2-0ccb043ac574 as it is not a pitching video.\n",
            "Processing row index: 10797, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=7bc8fa9e-6303-4a75-8fef-1c733fcb9957\n",
            "Error processing 7cc90da8-7e63-4074-bcbc-bc327be0d749: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id 7cc90da8-7e63-4074-bcbc-bc327be0d749 as it is not a pitching video.\n",
            "Processing row index: 12883, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=3a86a2d1-0845-4c10-82c6-cd2644baee20\n",
            "Error processing 7bc8fa9e-6303-4a75-8fef-1c733fcb9957: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id 7bc8fa9e-6303-4a75-8fef-1c733fcb9957 as it is not a pitching video.\n",
            "Processing row index: 14823, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=c2a1e26f-99af-4419-a266-3c60a3a70c05\n",
            "Error processing 3a86a2d1-0845-4c10-82c6-cd2644baee20: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id 3a86a2d1-0845-4c10-82c6-cd2644baee20 as it is not a pitching video.\n",
            "Processing row index: 15168, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=de7df3b9-80b7-4aad-8325-c1fe4dd794af\n",
            "Error processing c2a1e26f-99af-4419-a266-3c60a3a70c05: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id c2a1e26f-99af-4419-a266-3c60a3a70c05 as it is not a pitching video.\n",
            "Error processing c2a1e26f-99af-4419-a266-3c60a3a70c05: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 1 for play_id c2a1e26f-99af-4419-a266-3c60a3a70c05 as it is not a pitching video.\n",
            "Processing row index: 17872, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=bc166071-b486-4c3c-90b2-c2f08e12d3d9\n",
            "Error processing de7df3b9-80b7-4aad-8325-c1fe4dd794af: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id de7df3b9-80b7-4aad-8325-c1fe4dd794af as it is not a pitching video.\n",
            "Processing row index: 18826, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=342e1810-b817-43c9-af21-151f115e4010\n",
            "Error processing bc166071-b486-4c3c-90b2-c2f08e12d3d9: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id bc166071-b486-4c3c-90b2-c2f08e12d3d9 as it is not a pitching video.\n",
            "Error processing bc166071-b486-4c3c-90b2-c2f08e12d3d9: Array has fewer frames than the number of samples requested.\n",
            "Skipping segment 1 for play_id bc166071-b486-4c3c-90b2-c2f08e12d3d9 as it is not a pitching video.\n",
            "Processing row index: 19340, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=192ad661-9365-4d37-b004-5eabf057c8d9\n",
            "Error processing 342e1810-b817-43c9-af21-151f115e4010: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id 342e1810-b817-43c9-af21-151f115e4010 as it is not a pitching video.\n",
            "Processing row index: 20546, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=dbf38356-5826-4ad0-903c-5256124472f7\n",
            "Error processing 192ad661-9365-4d37-b004-5eabf057c8d9: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id 192ad661-9365-4d37-b004-5eabf057c8d9 as it is not a pitching video.\n",
            "Error processing 192ad661-9365-4d37-b004-5eabf057c8d9: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 1 for play_id 192ad661-9365-4d37-b004-5eabf057c8d9 as it is not a pitching video.\n",
            "Processing row index: 20674, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=1d19a8da-2cb0-4fea-ba29-caa910c8c807\n",
            "Error processing dbf38356-5826-4ad0-903c-5256124472f7: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id dbf38356-5826-4ad0-903c-5256124472f7 as it is not a pitching video.\n",
            "Error processing dbf38356-5826-4ad0-903c-5256124472f7: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 1 for play_id dbf38356-5826-4ad0-903c-5256124472f7 as it is not a pitching video.\n",
            "Processing row index: 22341, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=e2fd2ef1-2a9e-43c1-b1e1-01b737bb3919\n",
            "Error processing 1d19a8da-2cb0-4fea-ba29-caa910c8c807: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id 1d19a8da-2cb0-4fea-ba29-caa910c8c807 as it is not a pitching video.\n",
            "Processing row index: 23552, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=0401b272-3061-49da-b9af-b79b9c08e879\n",
            "Error processing e2fd2ef1-2a9e-43c1-b1e1-01b737bb3919: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id e2fd2ef1-2a9e-43c1-b1e1-01b737bb3919 as it is not a pitching video.\n",
            "Error processing e2fd2ef1-2a9e-43c1-b1e1-01b737bb3919: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 1 for play_id e2fd2ef1-2a9e-43c1-b1e1-01b737bb3919 as it is not a pitching video.\n",
            "Processing row index: 24314, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=20786c03-8ada-40df-9bfb-77e31d841f53\n",
            "Error processing 0401b272-3061-49da-b9af-b79b9c08e879: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id 0401b272-3061-49da-b9af-b79b9c08e879 as it is not a pitching video.\n",
            "Error processing 0401b272-3061-49da-b9af-b79b9c08e879: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 1 for play_id 0401b272-3061-49da-b9af-b79b9c08e879 as it is not a pitching video.\n",
            "Processing row index: 24413, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=760b2e58-f151-4da9-bc04-343009fbf50f\n",
            "Error processing 20786c03-8ada-40df-9bfb-77e31d841f53: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id 20786c03-8ada-40df-9bfb-77e31d841f53 as it is not a pitching video.\n",
            "Error processing 20786c03-8ada-40df-9bfb-77e31d841f53: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 1 for play_id 20786c03-8ada-40df-9bfb-77e31d841f53 as it is not a pitching video.\n",
            "Processing row index: 26240, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=e8e9bc7d-73c2-4f6f-86ef-ba40d7756953\n",
            "Error processing 760b2e58-f151-4da9-bc04-343009fbf50f: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id 760b2e58-f151-4da9-bc04-343009fbf50f as it is not a pitching video.\n",
            "Error processing 760b2e58-f151-4da9-bc04-343009fbf50f: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 1 for play_id 760b2e58-f151-4da9-bc04-343009fbf50f as it is not a pitching video.\n",
            "Processing row index: 27160, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=790fa685-fbf0-481d-ac27-79a074b2a603\n",
            "Error processing e8e9bc7d-73c2-4f6f-86ef-ba40d7756953: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id e8e9bc7d-73c2-4f6f-86ef-ba40d7756953 as it is not a pitching video.\n",
            "Error processing e8e9bc7d-73c2-4f6f-86ef-ba40d7756953: Array has fewer frames than the number of samples requested.\n",
            "Skipping segment 1 for play_id e8e9bc7d-73c2-4f6f-86ef-ba40d7756953 as it is not a pitching video.\n",
            "Processing row index: 27276, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=f4875474-b86f-4c58-9c04-de4c05ffcce8\n",
            "Error processing 790fa685-fbf0-481d-ac27-79a074b2a603: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id 790fa685-fbf0-481d-ac27-79a074b2a603 as it is not a pitching video.\n",
            "Error processing 790fa685-fbf0-481d-ac27-79a074b2a603: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 1 for play_id 790fa685-fbf0-481d-ac27-79a074b2a603 as it is not a pitching video.\n",
            "Processing row index: 27588, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=5a5c31bc-b6fe-4604-8a2e-d373e91f0576\n",
            "Error processing f4875474-b86f-4c58-9c04-de4c05ffcce8: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id f4875474-b86f-4c58-9c04-de4c05ffcce8 as it is not a pitching video.\n",
            "Error processing f4875474-b86f-4c58-9c04-de4c05ffcce8: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 1 for play_id f4875474-b86f-4c58-9c04-de4c05ffcce8 as it is not a pitching video.\n",
            "Processing row index: 29790, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=9604e10f-908d-4d1c-860c-92adb5a71587\n",
            "Error processing 5a5c31bc-b6fe-4604-8a2e-d373e91f0576: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id 5a5c31bc-b6fe-4604-8a2e-d373e91f0576 as it is not a pitching video.\n",
            "Error processing 5a5c31bc-b6fe-4604-8a2e-d373e91f0576: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 1 for play_id 5a5c31bc-b6fe-4604-8a2e-d373e91f0576 as it is not a pitching video.\n",
            "Processing row index: 31752, pitcher: May, Dustin(R), video_link: https://baseballsavant.mlb.com/sporty-videos?playId=fdfb9794-6806-4a5c-8064-5693bc6293a7\n",
            "Error processing 9604e10f-908d-4d1c-860c-92adb5a71587: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 0 for play_id 9604e10f-908d-4d1c-860c-92adb5a71587 as it is not a pitching video.\n",
            "Error processing 9604e10f-908d-4d1c-860c-92adb5a71587: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
            "Skipping segment 1 for play_id 9604e10f-908d-4d1c-860c-92adb5a71587 as it is not a pitching video.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-3283273293>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#process_videos_from_df_numpy(df, 0, 1300, output_folder+f\"cropping_videos/\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprocess_videos_from_df_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34mf\"cropping_videos/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-3844535128>\u001b[0m in \u001b[0;36mprocess_videos_from_df_numpy\u001b[0;34m(df, starting_row, n_rows, output_folder)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error in consumer for play_id {play_id}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproducer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsumer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#process_videos_from_df_numpy(df, 0, 1300, output_folder+f\"cropping_videos/\")\n",
        "process_videos_from_df_numpy(df, 0, 5, output_folder+f\"cropping_videos/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## To downlaod files to local compute\n",
        "# Step 1: Zip the folder containing your output videos\n",
        "shutil.make_archive(\"cropped_videos\", 'zip', \"Video Outputs/\")\n",
        "\n",
        "# Step 2: Download the zip file to your computer\n",
        "files.download(\"cropped_videos.zip\")"
      ],
      "metadata": {
        "id": "SQiLSlnCdB8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uNfkADcw5UH"
      },
      "outputs": [],
      "source": [
        "\n",
        "def process_video_from_url(video_page_url, output_folder, play_id_prefix=\"manual\"):\n",
        "    \"\"\"\n",
        "    Processes a single video URL: downloads, extracts, crops, and saves it.\n",
        "\n",
        "    Args:\n",
        "        video_page_url (str): URL to the video page.\n",
        "        output_folder (str): Directory to save the cropped videos.\n",
        "        play_id_prefix (str): Prefix for the play ID if not extracted from URL.\n",
        "    \"\"\"\n",
        "    if not video_page_url:\n",
        "        print(\"No video URL provided.\")\n",
        "        return\n",
        "\n",
        "    match = re.search(r\"playId=([a-z0-9\\-]+)\", video_page_url)\n",
        "    play_id = match.group(1) if match else play_id_prefix\n",
        "\n",
        "    print(f\"Processing video with play_id: {play_id}\")\n",
        "    video_url = get_video_url(play_id)\n",
        "    if not video_url:\n",
        "        print(f\"Could not retrieve video URL for play_id: {play_id}\")\n",
        "        return\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as tmp:\n",
        "        download_video(video_url, tmp.name)\n",
        "        temp_video_path = tmp.name\n",
        "\n",
        "    frames_array = video_to_numpy(temp_video_path)\n",
        "    print(f\"Frames array shape: {np.array(frames_array).shape}\")\n",
        "\n",
        "    gradient_array = gradient_analysis(frames_array)\n",
        "    cropped_frames = crop_video(frames_array, gradient_array)\n",
        "\n",
        "    for i, frames in enumerate(cropped_frames):\n",
        "        output_filename = os.path.join(output_folder, f\"cropped_{play_id}_{i}.mp4\")\n",
        "        output_vid = crop_video_size(np.array(frames), detect_and_draw_lowest_human)\n",
        "        convert_video(output_vid, output_filename)\n",
        "        print(f\"Cropped video saved to: {output_filename}\")\n",
        "\n",
        "    os.remove(temp_video_path)\n",
        "\n",
        "# Example usage: Process a single video URL\n",
        "video_page_url = \"https://baseballsavant.mlb.com/sporty-videos?playId=a726eb60-fc99-45b1-b57f-485d4ff95ce5\"\n",
        "output_folder = \"Video Outputs/\"\n",
        "\n",
        "process_video_from_url(video_page_url, output_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISCcUKUZw5UH"
      },
      "outputs": [],
      "source": [
        "\n",
        "x = np.array([1, 2, 3])\n",
        "t = torch.from_numpy(x)  # This should not raise an error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0M_Q9fSVhXC7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVpz9hfWh0lE"
      },
      "outputs": [],
      "source": [
        "mp_pose = mp.solutions.pose\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "pose_model = mp_pose.Pose(static_image_mode=False, model_complexity=1)\n",
        "\n",
        "def draw_pose(frame):\n",
        "    results = pose_model.process(frame)\n",
        "    if results.pose_landmarks:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            frame,\n",
        "            results.pose_landmarks,\n",
        "            mp_pose.POSE_CONNECTIONS,\n",
        "            mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2),\n",
        "            mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2)\n",
        "        )\n",
        "    return frame\n",
        "\n",
        "def save_video(output_path, frames, fps):\n",
        "    if not frames:\n",
        "        print(\"No frames to save.\")\n",
        "        return\n",
        "\n",
        "    h, w = frames[0].shape[:2]\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "\n",
        "    for frame in frames:\n",
        "        bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "        out.write(bgr)\n",
        "\n",
        "    out.release()\n",
        "    print(f\"✅ Saved pose-overlay video to: {output_path}\")\n",
        "\n",
        "def overlay_pose_and_save(video_path, output_path):\n",
        "    video_array = video_to_numpy(video_path)\n",
        "    posed_frames = []\n",
        "\n",
        "    for frame in video_array:\n",
        "        posed_frame = draw_pose(frame.copy())\n",
        "        posed_frames.append(posed_frame)\n",
        "\n",
        "    convert_video(np.array(posed_frames), output_path)\n",
        "    print(f\"Pose overlay video saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihuhDQNJcOXP"
      },
      "outputs": [],
      "source": [
        "# prompt: create a function that lists all the mp4 videos in a given directory that start with the word cropped\n",
        "import os\n",
        "from IPython.display import HTML, display\n",
        "from base64 import b64encode\n",
        "\n",
        "def list_cropped_mp4_videos(directory):\n",
        "  \"\"\"\n",
        "  Lists all MP4 video files in a given directory that start with 'cropped'.\n",
        "\n",
        "  Args:\n",
        "    directory (str): The path to the directory to search.\n",
        "\n",
        "  Returns:\n",
        "    list: A list of filenames (strings) that match the criteria.\n",
        "  \"\"\"\n",
        "  cropped_videos = []\n",
        "  if not os.path.isdir(directory):\n",
        "    print(f\"Error: Directory not found at {directory}\")\n",
        "    return cropped_videos\n",
        "\n",
        "  for filename in os.listdir(directory):\n",
        "    if filename.startswith('cropped') and filename.endswith('.mp4'):\n",
        "      cropped_videos.append(filename)\n",
        "\n",
        "  return cropped_videos\n",
        "\n",
        "\n",
        "def play_video_cv2(video_path, delay=30):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error opening video file\")\n",
        "        return\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        cv2_imshow(frame)\n",
        "        time.sleep(delay / 10000.0)  # delay in seconds\n",
        "    cap.release()\n",
        "\n",
        "# Example usage:\n",
        "cropped_video_files = list_cropped_mp4_videos(output_folder + \"cropped_videos/\")\n",
        "print(\"Cropped MP4 videos found:\")\n",
        "for video_file in cropped_video_files:\n",
        "  print(video_file)\n",
        "  # === USAGE ===\n",
        "  video_path = os.path.join(output_folder+\"cropped_videos/\", video_file)\n",
        "  video_array = video_to_numpy(video_path)\n",
        "  overlay_pose_and_save(video_path, os.path.join(output_folder+\"posed_videos/\", f\"posed_{video_file}\"))\n",
        "  #display(play_video_cv2(os.path.join(output_folder+\"posed_videos/\", f\"posed_{video_file}\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu022wyQw5UI"
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())  # True if CUDA is available\n",
        "print(torch.cuda.current_device())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "#put a torch tensor on the GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "# Example: Create a tensor and move it to the GPU\n",
        "tensor = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
        "# Check if the tensor is on the GPU\n",
        "print(f\"Tensor is on GPU: {tensor.is_cuda}\")\n",
        "# Check if the tensor is on the GPU\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}